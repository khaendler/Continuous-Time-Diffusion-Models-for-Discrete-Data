{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook\n",
    "\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lib.datasets.synthetic as synthetic\n",
    "import torchvision\n",
    "import lib.datasets.maze as maze \n",
    "import lib.datasets.mnist as mnist\n",
    "import torchvision.transforms as transforms \n",
    "import matplotlib.pyplot as plt\n",
    "import lib.utils.bookkeeping as bookkeeping\n",
    "from pathlib import Path\n",
    "import lib.models.models as models\n",
    "import lib.models.model_utils as model_utils\n",
    "import lib.datasets.dataset_utils as dataset_utils\n",
    "import lib.losses.losses as losses\n",
    "import lib.losses.losses_utils as losses_utils\n",
    "import lib.training.training as training\n",
    "import lib.training.training_utils as training_utils\n",
    "import lib.optimizers.optimizers as optimizers\n",
    "import lib.optimizers.optimizers_utils as optimizers_utils\n",
    "import lib.loggers.loggers as loggers\n",
    "import lib.loggers.logger_utils as logger_utils\n",
    "from lib.models.models import UniformRate, UniformVariantRate, GaussianTargetRate\n",
    "import lib.utils.utils as utils\n",
    "import numpy as np\n",
    "from lib.datasets import dataset_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "def show_images(images, n=8):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(1, n, i + 1)\n",
    "        plt.imshow(images[i].permute(1, 2, 0).numpy().astype(\"uint8\"), cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_samples(samples, im_size=0, axis=False, im_fmt=None):\n",
    "    \"\"\"Plot samples.\"\"\"\n",
    "    plt.scatter(samples[:, 0], samples[:, 1], marker=\".\")\n",
    "    plt.axis(\"equal\")\n",
    "    if im_size > 0:\n",
    "        plt.xlim(-im_size, im_size)\n",
    "        plt.ylim(-im_size, im_size)\n",
    "    if not axis:\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.maze_config.config_bert_maze import get_config\n",
    "cfg = get_config()\n",
    "model = model_utils.create_model(cfg, cfg.device)\n",
    "print(\"Number of Parameters: \", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.networks.ddsm_networks import ProteinScoreNet\n",
    "from config.maze_config.config_hollow_maze import get_config\n",
    "cfg = get_config()\n",
    "cfg.device = 'cuda'\n",
    "\n",
    "\n",
    "\n",
    "D = cfg.model.concat_dim = 1024\n",
    "S = cfg.data.S = 5\n",
    "B = cfg.data.batch_size\n",
    "net = ProteinScoreNet(cfg).to(cfg.device)\n",
    "ts = torch.rand((B,), device=cfg.device) * (1.0 - 0.01) + 0.01\n",
    "x = torch.randint(low=0, high=S, size=(B, D), device='cuda')\n",
    "x_out = net(x, ts)\n",
    "print(x_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.mnist_config.config_bert_mnist import get_config\n",
    "cfg = get_config()\n",
    "device = cfg.device \n",
    "device = 'cuda'\n",
    "mnist_dataset = dataset_utils.get_dataset(cfg, device, cfg.data.location)\n",
    "cfg.data.batch_size = 10\n",
    "mnist_dl = DataLoader(mnist_dataset,\n",
    "                                cfg.data.batch_size, shuffle=cfg.data.shuffle,\n",
    "                                num_workers=0)\n",
    "                                #worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.synthetic_config.config_ebm_synthetic import get_config\n",
    "cfg = get_config()\n",
    "location = \"lib/datasets/Synthetic/data_2spirals.npy\"\n",
    "device = cfg.device \n",
    "device = 'cuda'\n",
    "mnist_dataset = dataset_utils.get_dataset(cfg, device, location)\n",
    "cfg.data.batch_size = 1000\n",
    "mnist_dl = DataLoader(mnist_dataset,\n",
    "                                cfg.data.batch_size, shuffle=cfg.data.shuffle,\n",
    "                                num_workers=0)\n",
    "                                #worker_init_fn=worker_init_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remapping binary repr with gray code\n",
      "cuda:0\n",
      "Time points tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "tensor([[0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.3105, 0.6895],\n",
      "        [0.6895, 0.3105],\n",
      "        [0.6895, 0.3105]], device='cuda:0') torch.Size([320, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m noise_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(low\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, high\u001b[39m=\u001b[39mS, size\u001b[39m=\u001b[39m(B, D), device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m noise_x \u001b[39m=\u001b[39m synthetic\u001b[39m.\u001b[39mbin2float(noise_x\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint32), inv_bm, cfg\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconcat_dim, cfg\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mint_scale)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blise/home/pheller/Remote-Thesis/ContTimeDiscreteSpace/TAUnSDDM/maze_workaround.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m plot_samples(noise_x\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint32)) \u001b[39m# * 127.5\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "cfg.model.rate_const = 0.7 #0.00009\n",
    "cfg.model.t_func = \"log_sqr\"\n",
    "#model = UniformRate(cfg, device)\n",
    "model = UniformVariantRate(cfg, device)\n",
    "# model = GaussianTargetRate(cfg, device)\n",
    "S = 2\n",
    "bm, inv_bm = synthetic.get_binmap(cfg.model.concat_dim, cfg.data.binmode)\n",
    "\n",
    "for minibatch in mnist_dl:\n",
    "    print(minibatch.device)\n",
    "    B, D = minibatch.shape\n",
    "    minibatch = minibatch.view(B, D).to('cpu')\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "    ts = torch.ones((B,), device=device) *1\n",
    "    print(\"Time points\", ts[:9])\n",
    "\n",
    "    qt0 = model.transition(\n",
    "        ts\n",
    "    )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "    # R_t = beta_t * R_b\n",
    "    rate = model.rate(\n",
    "        ts\n",
    "    )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "\n",
    "    qt0_rows_reg = qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(\n",
    "            D\n",
    "        ),  # repeats every element 0 to B-1 D-times\n",
    "        minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "        :,\n",
    "    ]  # (B*D, S)\n",
    "    b = utils.expand_dims(torch.arange(B, device=device), (tuple(range(1, minibatch.dim()))))\n",
    "    qt0 = qt0[b, minibatch.long()]\n",
    "    print(qt0_rows_reg, qt0_rows_reg.shape)\n",
    "    # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "    x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "    x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "        B, D\n",
    "    )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "\n",
    "    rate_vals_square = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "    ]  # (B*D, S)\n",
    "\n",
    "    rate_vals_square[\n",
    "        torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "    ] = 0.0  # - values = 0 => in rate_vals_square[0, 1] = 0\n",
    "\n",
    "    rate_vals_square = rate_vals_square.view(B, D, S)  # (B*D, S) => (B, D, S)\n",
    "\n",
    "        #  Summe der Werte entlang der Dimension S\n",
    "    rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(\n",
    "        B, D\n",
    "    )  # B, D with every entry = S-1? => for entries of x_t same prob to transition?\n",
    "    square_dimcat = torch.distributions.categorical.Categorical(\n",
    "        rate_vals_square_dimsum\n",
    "    )\n",
    "\n",
    "\n",
    "    square_dims = square_dimcat.sample()  # (B,) taking values in [0, D)\n",
    "\n",
    "    rate_new_val_probs = rate_vals_square[\n",
    "        torch.arange(B, device=device), square_dims, :\n",
    "    ]  # (B, S) => every row has only one entry = 0, everywhere else 1; chooses the row square_dim of rate_vals_square\n",
    "    # => now rate_new_val_probs: (B, S) with every row (1, 1, 0)\n",
    "\n",
    "    # samples from rate_new_val_probs and chooses state to transition to => more likely where entry is 1 instead of 0?\n",
    "    square_newvalcat = torch.distributions.categorical.Categorical(\n",
    "        rate_new_val_probs\n",
    "    )\n",
    "\n",
    "    square_newval_samples = (\n",
    "        square_newvalcat.sample()\n",
    "    )  # (B, ) taking values in [0, S)\n",
    "\n",
    "\n",
    "    x_tilde = x_t.clone()\n",
    "    x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "    x_tilde = synthetic.bin2float(x_tilde.detach().cpu().numpy().astype(np.int32), inv_bm, cfg.model.concat_dim, cfg.data.int_scale)\n",
    "    minibatch = synthetic.bin2float(minibatch.detach().cpu().numpy().astype(np.int32), inv_bm, cfg.model.concat_dim, cfg.data.int_scale)\n",
    "    plot_samples(minibatch) # * 127.5\n",
    "    plot_samples(x_tilde) # * 127.5\n",
    "    noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "    noise_x = synthetic.bin2float(noise_x.detach().cpu().numpy().astype(np.int32), inv_bm, cfg.model.concat_dim, cfg.data.int_scale)\n",
    "    plot_samples(noise_x) # * 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.rate_const = 0.1 #0.00009\n",
    "cfg.model.t_func = \"log\"\n",
    "\n",
    "cfg.model.time_base = 3\n",
    "cfg.model.time_exp = 100\n",
    "cfg.model.rate_sigma = 6.0\n",
    "cfg.model.Q_sigma = 512.0\n",
    "#model = UniformRate(cfg, device)\n",
    "model = UniformVariantRate(cfg, device)\n",
    "# model = GaussianTargetRate(cfg, device)\n",
    "S = 256\n",
    "\n",
    "\n",
    "for minibatch in mnist_dl:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    print(minibatch.device)\n",
    "    D = C*H*W\n",
    "    minibatch = minibatch.view(B, D).to('cpu')\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "    ts = torch.ones((B,), device=device) *1\n",
    "    print(\"Time points\", ts[:9])\n",
    "\n",
    "    qt0 = model.transition(\n",
    "        ts\n",
    "    )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "    # R_t = beta_t * R_b\n",
    "    rate = model.rate(\n",
    "        ts\n",
    "    )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "\n",
    "    qt0_rows_reg = qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(\n",
    "            D\n",
    "        ),  # repeats every element 0 to B-1 D-times\n",
    "        minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "        :,\n",
    "    ]  # (B*D, S)\n",
    "    b = utils.expand_dims(torch.arange(B, device=device), (tuple(range(1, minibatch.dim()))))\n",
    "    qt0 = qt0[b, minibatch.long()]\n",
    "    print(qt0_rows_reg, qt0_rows_reg.shape)\n",
    "    # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "    x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "    x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "        B, D\n",
    "    )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "\n",
    "    rate_vals_square = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "    ]  # (B*D, S)\n",
    "\n",
    "    rate_vals_square[\n",
    "        torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "    ] = 0.0  # - values = 0 => in rate_vals_square[0, 1] = 0\n",
    "\n",
    "    rate_vals_square = rate_vals_square.view(B, D, S)  # (B*D, S) => (B, D, S)\n",
    "\n",
    "        #  Summe der Werte entlang der Dimension S\n",
    "    rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(\n",
    "        B, D\n",
    "    )  # B, D with every entry = S-1? => for entries of x_t same prob to transition?\n",
    "    square_dimcat = torch.distributions.categorical.Categorical(\n",
    "        rate_vals_square_dimsum\n",
    "    )\n",
    "\n",
    "\n",
    "    square_dims = square_dimcat.sample()  # (B,) taking values in [0, D)\n",
    "\n",
    "    rate_new_val_probs = rate_vals_square[\n",
    "        torch.arange(B, device=device), square_dims, :\n",
    "    ]  # (B, S) => every row has only one entry = 0, everywhere else 1; chooses the row square_dim of rate_vals_square\n",
    "    # => now rate_new_val_probs: (B, S) with every row (1, 1, 0)\n",
    "\n",
    "    # samples from rate_new_val_probs and chooses state to transition to => more likely where entry is 1 instead of 0?\n",
    "    square_newvalcat = torch.distributions.categorical.Categorical(\n",
    "        rate_new_val_probs\n",
    "    )\n",
    "\n",
    "    square_newval_samples = (\n",
    "        square_newvalcat.sample()\n",
    "    )  # (B, ) taking values in [0, S)\n",
    "\n",
    "\n",
    "    x_tilde = x_t.clone()\n",
    "    x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "    x_tilde = x_tilde.view(B, C, H, W)\n",
    "\n",
    "    show_images(minibatch.view(B, C, H, W).detach().cpu(), n=9) # * 127.5\n",
    "    show_images(x_tilde.detach().cpu(), n=9) # * 127.5\n",
    "    noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "    show_images(noise_x.view(B, C, H, W).detach().cpu(), n=9) # * 127.5\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.maze_config.config_hollow_maze import get_config\n",
    "cfg = get_config()\n",
    "device = cfg.device \n",
    "device = 'cuda'\n",
    "maze_dataset = dataset_utils.get_dataset(cfg, device)\n",
    "cfg.data.batch_size = 10\n",
    "maze_dl = DataLoader(maze_dataset,\n",
    "                                cfg.data.batch_size, shuffle=cfg.data.shuffle,\n",
    "                                num_workers=0)\n",
    "                                #worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.rate_const = 2.3 # 2.3 log_sqr t=0.5T 0.4763, 0.2619, 0.2619\n",
    "cfg.model.t_func = \"log_sqr\"\n",
    "cfg.model.time_base = 1\n",
    "cfg.model.time_exp = 250\n",
    "#model = UniformRate(cfg, 'cuda')\n",
    "model = UniformVariantRate(cfg, 'cuda')\n",
    "S = 3\n",
    "\n",
    "\n",
    "for minibatch in maze_dl:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    print(minibatch.device)\n",
    "    D = C*H*W\n",
    "    minibatch = minibatch.view(B, D)\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "    ts = torch.ones((B,), device=device) * 1\n",
    "    #ts = torch.linspace(0.01, 1, B, device=device)\n",
    "    print(\"Time points\", ts[:9])\n",
    "\n",
    "    qt0 = model.transition(\n",
    "        ts\n",
    "    )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "    # R_t = beta_t * R_b\n",
    "    rate = model.rate(\n",
    "        ts\n",
    "    )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "\n",
    "    qt0_rows_reg = qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(\n",
    "            D\n",
    "        ),  # repeats every element 0 to B-1 D-times\n",
    "        minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "        :,\n",
    "    ]  # (B*D, S)\n",
    "    b = utils.expand_dims(torch.arange(B, device=device), (tuple(range(1, minibatch.dim()))))\n",
    "    qt0 = qt0[b, minibatch.long()].view(-1, S)\n",
    "    print(qt0_rows_reg)# , qt0)\n",
    "    # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "    x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "    x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "        B, D\n",
    "    )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "\n",
    "    rate_vals_square = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "    ]  # (B*D, S)\n",
    "\n",
    "    rate_vals_square[\n",
    "        torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "    ] = 0.0  # - values = 0 => in rate_vals_square[0, 1] = 0\n",
    "\n",
    "    rate_vals_square = rate_vals_square.view(B, D, S)  # (B*D, S) => (B, D, S)\n",
    "\n",
    "        #  Summe der Werte entlang der Dimension S\n",
    "    rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(\n",
    "        B, D\n",
    "    )  # B, D with every entry = S-1? => for entries of x_t same prob to transition?\n",
    "    square_dimcat = torch.distributions.categorical.Categorical(\n",
    "        rate_vals_square_dimsum\n",
    "    )\n",
    "\n",
    "\n",
    "    square_dims = square_dimcat.sample()  # (B,) taking values in [0, D)\n",
    "\n",
    "    rate_new_val_probs = rate_vals_square[\n",
    "        torch.arange(B, device=device), square_dims, :\n",
    "    ]  # (B, S) => every row has only one entry = 0, everywhere else 1; chooses the row square_dim of rate_vals_square\n",
    "    # => now rate_new_val_probs: (B, S) with every row (1, 1, 0)\n",
    "\n",
    "    # samples from rate_new_val_probs and chooses state to transition to => more likely where entry is 1 instead of 0?\n",
    "    square_newvalcat = torch.distributions.categorical.Categorical(\n",
    "        rate_new_val_probs\n",
    "    )\n",
    "\n",
    "    square_newval_samples = (\n",
    "        square_newvalcat.sample()\n",
    "    )  # (B, ) taking values in [0, S)\n",
    "\n",
    "\n",
    "    x_tilde = x_t.clone()\n",
    "    x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "    x_tilde = x_tilde.view(B, C, H, W)\n",
    "\n",
    "    show_images(minibatch.view(B, C, H, W).detach().cpu()* 127.5, n=B) # * 127.5\n",
    "    show_images(x_tilde.detach().cpu() * 127.5, n=B) # * 127.5\n",
    "    noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "    show_images(noise_x.view(B, C, H, W).detach().cpu() * 127.5, n=B) # * 127.5\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for minibatch in maze_dl:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    show_images(minibatch.view(B, C, H, W).detach().cpu()* 127.5, n=8) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "D= 1\n",
    "S = 256\n",
    "x = torch.randint(low=0, high=S, size=(B, D))\n",
    "print(x.shape)\n",
    "x = F.one_hot(x.long(), S)\n",
    "out = x.permute(0, 2, 1).float()\n",
    "print(out.shape, type(out))\n",
    "#out = x\n",
    "lin = nn.Conv1d(S, 256*2, kernel_size=9, padding=4)\n",
    "print(lin(out).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.utils.utils as utils\n",
    "cfg.model.rate_const = 0.5\n",
    "cfg.model.t_func = \"log\"\n",
    "cfg.model.time_base = 5\n",
    "cfg.model.time_exp = 5\n",
    "model = UniformRate(cfg, 'cuda')\n",
    "model = UniformVariantRate(cfg, 'cuda')\n",
    "device = 'cpu'\n",
    "S = 3\n",
    "min_time = 0.01\n",
    "\n",
    "\n",
    "for minibatch in mnist_dataset:\n",
    "    \n",
    "\n",
    "    if len(minibatch.shape) == 4:\n",
    "        B, C, H, W = minibatch.shape\n",
    "        minibatch = minibatch.view(B, C * H * W)\n",
    "    # hollow xt, t, l_all, l_xt geht rein\n",
    "    B = minibatch.shape[0]\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - min_time) + min_time\n",
    "    ts = torch.ones((B,)) * 1\n",
    "    print(ts[:9])\n",
    "    #\n",
    "\n",
    "    qt0 = model.transition(ts)  # (B, S, S)\n",
    "\n",
    "    # rate = model.rate(ts)  # (B, S, S)\n",
    "\n",
    "    b = utils.expand_dims(torch.arange(B, device=device), (tuple(range(1, minibatch.dim()))))\n",
    "    qt0 = qt0[b, minibatch.long()]\n",
    "\n",
    "    # log loss\n",
    "    log_qt0 = torch.where(qt0 <= 0.0, -1e9, torch.log(qt0))\n",
    "    x_tilde = torch.distributions.categorical.Categorical(\n",
    "        logits=log_qt0\n",
    "    ).sample()  # bis hierhin <1 sek\n",
    "\n",
    "    x_tilde = x_tilde.view(B, C, H, W)\n",
    "    print(torch.mean(x_tilde[1,:, :, :].float()))\n",
    "    #print(x_tilde[0,0, :, :].std())\n",
    "    show_images(minibatch.view(B, C, H, W) * 127.5, n=9)\n",
    "    show_images(x_tilde * 127.5, n=9)\n",
    "    noise_x = torch.randint(low=0, high=S, size=(B, D), device=device)\n",
    "    show_images(noise_x.view(B, C, H, W) * 127.5, n=9)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int log(t**2 + 1)\n",
    "t = np.linspace(0.01, 1, 1000)\n",
    "f = 2 * t / (t**2 + 1)\n",
    "f_int = np.log(t**2 + 1)\n",
    "f_cos = np.sin(t) / np.sqrt(np.cos(t))\n",
    "a = 5\n",
    "b = 5\n",
    "f_log = a * np.log(b) * b**t\n",
    "plt.plot(f, label='log sqr')\n",
    "plt.plot(f_int, label='int log sqr')\n",
    "plt.plot(f_cos, label='cos')\n",
    "plt.plot(f_log, label='log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
