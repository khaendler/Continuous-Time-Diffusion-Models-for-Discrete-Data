{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lib.utils.bookkeeping as bookkeeping\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from ruamel.yaml.scalarfloat import ScalarFloat\n",
    "from config.synthetic_config.config_tauMLP_synthetic import get_config\n",
    "from torch.utils.data import DataLoader\n",
    "from lib.datasets import mnist, maze, protein, synthetic\n",
    "import lib.sampling.sampling as sampling\n",
    "import lib.sampling.sampling_utils as sampling_utils\n",
    "import lib.models.models as models\n",
    "import lib.models.model_utils as model_utils\n",
    "import lib.datasets.dataset_utils as dataset_utils\n",
    "from lib.datasets.metrics import eval_mmd\n",
    "\n",
    "def main():\n",
    "    \n",
    "    script_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    save_location = os.path.join(script_dir, \"SavedModels/Synthetic/\")\n",
    "    dataset_location = os.path.join(script_dir, cfg.data.location)\n",
    "\n",
    "    # creating paths\n",
    "    path = 'SavedModels/Synthetic/' # 'SavedModels/MAZE/' 'SavedModels/MNIST/'\n",
    "    date = '2023-11-16' # 2023-10-30 'Hollow-2023-10-29'\n",
    "    config_name = 'config_001_tauMLP.yaml' # 'config_001_maze.yaml' 'config_001_rate001.yaml'\n",
    "    model_name = 'model_199999_tauMLP.pt' # 'model_55999_rate001.pt' 'model_5999_maze.pt'\n",
    "\n",
    "    #config_name = 'config_001_r07.yaml' \n",
    "    #model_name = 'model_84999_hollowr07.pt' \n",
    "    config_path = os.path.join(path, date, config_name)\n",
    "    checkpoint_path = os.path.join(path, date, model_name)\n",
    "\n",
    "    # creating models\n",
    "    cfg = bookkeeping.load_config(config_path)\n",
    "    cfg.sampler.name = 'ElboLBJF' #'ExactSampling' # ElboLBJF CRMTauL CRMLBJF\n",
    "    cfg.logit_type = 'direct'\n",
    "    cfg.sampler.num_corrector_steps = 10\n",
    "    cfg.sampler.corrector_entry_time = ScalarFloat(0.1)\n",
    "    cfg.sampler.num_steps = 750\n",
    "    cfg.sampler.is_ordinal = True\n",
    "\n",
    "    #print(cfg)\n",
    "    device = torch.device(cfg.device)\n",
    "\n",
    "    model = model_utils.create_model(cfg, device)\n",
    "    print(\"number of parameters: \", sum([p.numel() for p in model.parameters()]))\n",
    "\n",
    "    #modified_model_state = utils.remove_module_from_keys(loaded_state['model'])\n",
    "    #model.load_state_dict(modified_model_state)\n",
    "    #optimizer = optimizers_utils.get_optimizer(model.parameters(), cfg)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), cfg.optimizer.lr)\n",
    "\n",
    "    sampler = sampling_utils.get_sampler(cfg)\n",
    "\n",
    "    state = {\"model\": model, \"optimizer\": optimizer, \"n_iter\": 0}\n",
    "    state = bookkeeping.load_state(state, checkpoint_path)\n",
    "    state['model'].eval()\n",
    "\n",
    "    dataset = dataset_utils.get_dataset(cfg, device, dataset_location)\n",
    "    dataloader = DataLoader(dataset, batch_size=cfg.data.batch_size, shuffle=cfg.data.shuffle)\n",
    "\n",
    "    n_samples = 1024\n",
    "    n_rounds = 1\n",
    "    mmd = eval_mmd(cfg, sampler, dataloader, n_rounds, num_samples=n_samples)\n",
    "    print(\"MMD\", mmd)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
