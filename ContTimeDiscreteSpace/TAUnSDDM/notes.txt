Rechnerisch Rate: 1.54 nach Sebastian
Minimizing L_CT loss => wird nicht 0, da abhängig von Rate
Je kleiner die Rate desto kleiner der loss => heißt nicht dass besser lernt
=> R=0.001 schlechter nach 3000 iter als R=0.7

bei CatRM:
Loss bei geringerer Rate auch geringer bei Maze: 0.001, 0.1, 1.5 
Plots bei 0.001 am schlechtesten bzw. R=1.54
=> reicht nicht Endverteilung anzusehen; vll am Ende bei t=T nahe zu Uniform Rate. aber bei t=0.5*T auch
Loss aber nicht abhängig von Rate?

MAZE: Je nach Performance: log_sqr mit Rate=2.3
MNIST Images: GaussianRate sigma_0 = 512, sigma_r = 6, beta(t) = 3*100^t log(100)

Tau schlechter als LBJF?

Maze: 
Hollow ab 200999 => mazes from 97, 97 to 99, 98
Problem Evaluierung:
Nur ein Solution Path
Solution Path gefunden und nur diesen Markiert

Problem Modell Architekturen:
Hollow muss immer Transformer-artig sein wegen Masking
Masked: MLP, Transformer, ResNet
CT-ELBO: Alles

Trade-Off: Flexibility und Speed:
Ursprüngliche Idee:
Masked Models und ELBO gleiches Modell und Hollow=Hollow
Maze => Unet gut:
Masked Modell kein Unet möglich?=> Nehme 12-layer Bert 
=> Schlechter als Hollow: Vll ist Architektur aber einfach nur schlechter und ich 
könnte ja auch ein BiDir nehmen?

=> Wenn ich aber bei allen gleiche Modell nehme, 
ja auch irgendwie doof => will ja flexibility nutzen

MNIST: 
CT-ELBO: Unet
Masked Model: 12-layer Bert?
Hollow: Hollow


Archi Fragen:
Kann ich Unet für Masked Models?
Kann ich BiDir für Masked Models? => theoretisch ja bringt 
aber nicht wirklich Mehrwert: Mache ja 2 Richtungen nur wegen Masking 

Bert nochmal mit tiefere Hidden Dim und weniger ResBlock layers? => heute nacht


Synthetic:
EBM: Stärke bei schwierigen Daten durch Flexibilität, 
aber genau da auch  Nachteil, da meist schwierige Datem hohe D und C und EBM O(DxC)

