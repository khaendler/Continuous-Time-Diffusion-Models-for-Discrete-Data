{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 11:35:15.321406: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNow, when we minimize LCT, we are sampling (x, x ̃) from the forward process and then maximizing the assigned model probability for \\n\\nthe pairing in the reverse direction, just as in LDT. The slight extra complexity comes from the fact we areconsidering the case \\n\\nwhen xk = xk+1 and the case when xk ̸= xk+1 separately. When xk = xk+1, this corresponds to the first term in LCT which we can see \\n\\nis minimizing the reverse rate out of x which is exactly maximizing the model probability for no transition to occur. When xk ̸= xk+1, \\n\\nthis corresponds to the second term in LCT, which is maximizing the reverse rate from x ̃ to x which in turn maximizes the model probability \\n\\nfor the x ̃ to x transition to occur.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from lib.models.models import UniformRate, UniformVariantRate\n",
    "\n",
    "from config.config_hollow_synthetic import get_config\n",
    "from lib.utils import utils\n",
    "import torch.nn.functional as F\n",
    "from lib.networks.unet import UNet\n",
    "from lib.networks.hollow_networks import BidirectionalTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "from lib.datasets.datasets import (\n",
    "    create_train_discrete_mnist_dataloader,\n",
    "    get_binmnist_datasets,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import ml_collections\n",
    "import yaml\n",
    "import lib.utils.bookkeeping as bookkeeping\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ssl\n",
    "import os\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import lib.models.models as models\n",
    "import lib.models.model_utils as model_utils\n",
    "import lib.datasets.datasets as datasets\n",
    "import lib.datasets.dataset_utils as dataset_utils\n",
    "import lib.losses.losses as losses\n",
    "import lib.losses.losses_utils as losses_utils\n",
    "import lib.training.training as training\n",
    "import lib.training.training_utils as training_utils\n",
    "import lib.optimizers.optimizers as optimizers\n",
    "import lib.optimizers.optimizers_utils as optimizers_utils\n",
    "import lib.loggers.loggers as loggers\n",
    "import lib.loggers.logger_utils as logger_utils\n",
    "import lib.sampling.sampling as sampling\n",
    "import lib.sampling.sampling_utils as sampling_utils\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import lib.sampling.sampling_utils as sampling_utils\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Now, when we minimize LCT, we are sampling (x, x ̃) from the forward process and then maximizing the assigned model probability for \n",
    "\n",
    "the pairing in the reverse direction, just as in LDT. The slight extra complexity comes from the fact we areconsidering the case \n",
    "\n",
    "when xk = xk+1 and the case when xk ̸= xk+1 separately. When xk = xk+1, this corresponds to the first term in LCT which we can see \n",
    "\n",
    "is minimizing the reverse rate out of x which is exactly maximizing the model probability for no transition to occur. When xk ̸= xk+1, \n",
    "\n",
    "this corresponds to the second term in LCT, which is maximizing the reverse rate from x ̃ to x which in turn maximizes the model probability \n",
    "\n",
    "for the x ̃ to x transition to occur.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = '/Users/paulheller/PythonRepositories/Master-Thesis/ContTimeDiscreteSpace/TAUnSDDM/'\n",
    "save_location = os.path.join(script_dir, 'SavedModels/Synthetic/')\n",
    "save_location_png = os.path.join(save_location, 'PNGs/')\n",
    "dataset_location = os.path.join(script_dir, 'lib/datasets/Synthetic/data_2spirals.npy')\n",
    "\n",
    "\n",
    "cfg = get_config()\n",
    "\n",
    "device = cfg.device\n",
    "dataset = dataset_utils.get_dataset(cfg, device, dataset_location)\n",
    "dataloader = DataLoader(dataset,\n",
    "    batch_size=cfg.data.batch_size,\n",
    "    shuffle=cfg.data.shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False,  ...,  True,  True, False],\n",
      "        [ True, False, False,  ..., False, False,  True],\n",
      "        [False, False, False,  ..., False, False,  True],\n",
      "        ...,\n",
      "        [ True, False,  True,  ..., False, False,  True],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [ True, False, False,  ..., False, False, False]]) <class 'torch.Tensor'> torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    print(i, type(i), i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg =  get_config()\n",
    "cfg.model.t_func = \"log_sqr\"\n",
    "cfg.model.rate_const = 0.01\n",
    "dataloader = create_train_discrete_mnist_dataloader(\n",
    "        batch_size=cfg.data.batch_size, image_size=cfg.data.image_size, use_augmentation=cfg.data.use_augm\n",
    "    )\n",
    "#train_set, _, _ = get_binmnist_datasets('/Users/paulheller/PythonRepositories/Master-Thesis/ContTimeDiscreteSpace/TAUnSDDM/lib/datasets/', device=\"cpu\")\n",
    "#dataloader = DataLoader(train_set, batch_size=cfg.data.batch_size, shuffle=True, num_workers=4)\n",
    "model = UniformRate(cfg, 'cpu')\n",
    "#model = UniformVariantRate(cfg, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist_batch(batch, save_path=None):\n",
    "    \"\"\"Plottet ein Batch von MNIST-Bildern.\"\"\"\n",
    "    num_images = batch.shape[0]\n",
    "    sqrt_num_images = int(np.sqrt(num_images))\n",
    "\n",
    "    fig, axes = plt.subplots(sqrt_num_images, sqrt_num_images, figsize=(8, 8))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = batch[i].squeeze()\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "        ax.axis(\"off\") \n",
    "\n",
    "    plt.tight_layout() \n",
    "    #plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "x_mean = 0 # 127.5\n",
    "x_std = 0 # 73.7\n",
    "\n",
    "for minibatch, _ in dataloader:\n",
    "    \n",
    "    B, C, H, W = minibatch.shape\n",
    "    minibatch = minibatch.view(B, C * H * W) \n",
    "    # hollow xt, t, l_all, l_xt geht rein\n",
    "    device = cfg.device\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.0\n",
    "    ts = torch.ones((B, )) \n",
    "    qt0 = model.transition(ts)  # (B, S, S)\n",
    "\n",
    "    # rate = model.rate(ts)  # (B, S, S)\n",
    "\n",
    "    b = utils.expand_dims(torch.arange(B), (tuple(range(1, minibatch.dim()))))\n",
    "    qt0 = qt0[b, minibatch.long()]\n",
    "\n",
    "    # log loss\n",
    "    log_qt0 = torch.where(qt0 <= 0.0, -1e9, torch.log(qt0))\n",
    "    xt = torch.distributions.categorical.Categorical(logits=log_qt0).sample()\n",
    "    xt = xt.view(B, C, H, W) \n",
    "    #print(type(xt))\n",
    "    plot_mnist_batch(xt)\n",
    "\n",
    "    print(torch.mean(xt[0,:, :, :].float()))\n",
    "    x_mean +=torch.mean(xt[0,:, :, :].float())\n",
    "    x_std += torch.std(xt[0,:, :, :].float())\n",
    "    i = i+1\n",
    "    print(\"Mean:\", x_mean / i)\n",
    "    print(\"Std:\", x_std / i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # if 4 Dim => like images: True\n",
    "i = 0\n",
    "x_mean_t = 0\n",
    "for minibatch, _ in dataloader:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    minibatch = minibatch.view(B, C * H * W)\n",
    "\n",
    "    B, D = minibatch.shape\n",
    "    device = model.device\n",
    "\n",
    "    # get random timestep between 1.0 and self.min_time\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "\n",
    "    qt0 = model.transition(\n",
    "        ts\n",
    "    )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "    # R_t = beta_t * R_b\n",
    "    rate = model.rate(\n",
    "        ts\n",
    "    )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "    # --------------- Sampling x_t, x_tilde --------------------\n",
    "    # qt0_rows_reg = (B * D, S) probability distribution\n",
    "    # diagonal elements of qt0 (higher probability) will be put at column of value of x_t\n",
    "    # we do this because then we sample from qt0_rows_reg and then it is most likely more similar to x0=batch\n",
    "    # example: q_t0 =   [0.4079, 0.2961, 0.2961],\n",
    "    #                   [0.2961, 0.4079, 0.2961],\n",
    "    #                   [0.2961, 0.2961, 0.4079]],\n",
    "    # batch = (2, 0, 1)\n",
    "    # qt0_rows_reg = [0.2961, 0.2961, 0.4079],\n",
    "    #                [0.4079, 0.2961, 0.4079],\n",
    "    #                [0.2961, 0.4079, 0.2961]\n",
    "\n",
    "    qt0_rows_reg = qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(\n",
    "            D\n",
    "        ),  # repeats every element 0 to B-1 D-times\n",
    "        minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "        :,\n",
    "    ]  # (B*D, S)\n",
    "\n",
    "    # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "    x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "    x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "        B, D\n",
    "    )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "    #print(torch.mean(x_t[0, :].float()))\n",
    "    x_mean_t +=torch.mean(x_t[0,:].float())\n",
    "    i = i+1\n",
    "    print(\"overall\", x_mean_t / i)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "B = 2\n",
    "D = 4\n",
    "S = 3\n",
    "log_prob = torch.rand((B,D, S))\n",
    "print(log_prob)\n",
    "x = torch.randint(low=0, high=3, size=(B, 1), device='cpu')\n",
    "one_hot = F.one_hot(x)\n",
    "print(one_hot,one_hot.shape)\n",
    "mult = log_prob * one_hot\n",
    "print(mult, mult.shape)\n",
    "sum_mult = torch.sum(mult, dim=-1)\n",
    "print(sum_mult, sum_mult.shape)\n",
    "\n",
    "print(torch.sum(sum_mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config =  get_config()\n",
    "bdt = BidirectionalTransformer(config)\n",
    "print(\"number of parameters: \", sum([p.numel() for p in bdt.parameters()]))\n",
    "B = config.data.batch_size\n",
    "D = config.concat_dim\n",
    "S = config.data.S\n",
    "xt= torch.randint(low=0, high=S, size=(B, D), dtype=torch.int)\n",
    "t = 1 * torch.ones((B, ))\n",
    "print(len(t.shape))\n",
    "# x_pred = bdt(xt, t)\n",
    "#print(x_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config =  get_config()\n",
    "model = UniformRate(config, 'cpu')\n",
    "S = 256\n",
    "B = 32\n",
    "D = 1024\n",
    "y = torch.randint(low=0, high=S, size=(B, D), dtype=torch.int)\n",
    "t = 1 * torch.ones((B, ))\n",
    "rate = model.rate_mat(y, t)\n",
    "print(rate.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "S=256\n",
    "rate_const = 1\n",
    "\n",
    "\n",
    "cfg.data.S = S\n",
    "device = 'cpu'\n",
    "cfg.model.rate_const = rate_const\n",
    "\n",
    "S = 256\n",
    "B = 64\n",
    "D = 1024\n",
    "model = UniformRate(cfg, 'cpu')\n",
    "unet = UNet(\n",
    "                in_channel=1,\n",
    "                out_channel=1,\n",
    "                channel=32,\n",
    "                channel_multiplier=cfg.model.ch_mult,\n",
    "                n_res_blocks=cfg.model.num_res_blocks,\n",
    "                attn_resolutions=[16],\n",
    "                num_heads=1,\n",
    "                dropout=cfg.model.dropout,\n",
    "                model_output = 'logits',  # 'logits' or 'logistic_pars'\n",
    "                num_classes=S,\n",
    "                x_min_max=(0, 255),\n",
    "                img_size=32,\n",
    "        )\n",
    "t = 1\n",
    "xt= torch.randint(low=0, high=S, size=(B, D), dtype=torch.int)\n",
    "B, D = xt.shape\n",
    "C, H, W = (1, 32, 32)\n",
    "S = 256\n",
    "x = xt.view(B, C, H, W)\n",
    "x_pred = unet(x, t * torch.ones((B,), device=device))\n",
    "x_pred = x_pred.view(B, D, S)\n",
    "print(\"PRED\",x_pred, x_pred.shape)\n",
    "log_p0t = F.log_softmax(x_pred, dim=2)\n",
    "print(log_p0t, log_p0t.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def transformer_timestep_embedding(timesteps, embedding_dim, max_positions=10000):\n",
    "    assert embedding_dim % 2 == 0\n",
    "    assert len(timesteps.shape) == 1\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(max_positions) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = timesteps[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "x = torch.randint(0, S, (B, D))  # Zufällige Integer zwischen 0 und vocab_size - 1\n",
    "\n",
    "# Embedding-Layer\n",
    "embedding = nn.Embedding(S, 10)\n",
    "x = embedding(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "x = torch.randint(0, S, (B, D))  # Zufällige Integer zwischen 0 und vocab_size - 1\n",
    "\n",
    "# Embedding-Layer\n",
    "embedding = nn.Embedding(S, 10)\n",
    "x = embedding(x)\n",
    "print(x.shape)\n",
    "\n",
    "t =1 * torch.ones((B,))\n",
    "temb = transformer_timestep_embedding(t, 10, max_positions=10000)\n",
    "\n",
    "assert x.ndim == 3 and temb.ndim == 2 # B, D, E and B, E\n",
    "print(temb.shape)\n",
    "temb = temb.unsqueeze(1)\n",
    "print(temb.shape)\n",
    "conditioner = temb\n",
    "\n",
    "#conditioner = torch.cat([conditioner, temb], dim=1) # B, 2D, E\n",
    "print(conditioner.shape)\n",
    "cond_dim = conditioner.size(1) # 2D\n",
    "print(\"cond_dim\", cond_dim)\n",
    "concat_dim = x.size(1) + cond_dim - 1 # 3D -1\n",
    "print(\"conc dim\", concat_dim)\n",
    "pos_idx = torch.arange(concat_dim, dtype=torch.int32).unsqueeze(0)\n",
    "print(\"pos\", pos_idx.shape)\n",
    "print(\"x[:, :-1]\", x[:, :-1].shape)\n",
    "x = torch.cat([conditioner, x[:, :-1]], dim=1)\n",
    "print(\"conditioner\", conditioner.shape)\n",
    "print(\"x\", x.shape)\n",
    "mask = pos_idx.unsqueeze(-1) <= pos_idx.unsqueeze(-2)\n",
    "mask = mask.unsqueeze(-3) \n",
    "print(\"mask\", mask, mask.shape)\n",
    "mask = mask[:, :, -cond_dim:, -cond_dim:] = 1.0\n",
    "print(\"mask\", mask, mask.shape)\n",
    "#attn_mask = torch.triu(torch.ones((concat_dim, concat_dim)), diagonal=1).bool()\n",
    "attn_mask = torch.tril(torch.ones(concat_dim, concat_dim), diagonal=1).bool()\n",
    "print(\"mask\", mask, mask.shape)\n",
    "print(mask.squeeze(0).squeeze(0) == ~attn_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = torch.triu(torch.ones((5, 5)), diagonal=1).bool()\n",
    "print(attn_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = torch.tril(torch.ones(5, 5), diagonal=-1).bool()\n",
    "print(attn_mask)\n",
    "#print(attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt_test = model.transition\n",
    "print(qt_test.shape)\n",
    "qt_test = utils.expand_dims(qt_test, axis=list(range(1, xt.dim() - 1)))\n",
    "print(qt_test.shape)\n",
    "torch.where(qt_test <= 1e-35, -1e9, torch.log(qt_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.01\n",
    "start_opt = time.time()\n",
    "t_eps = t - h #tau\n",
    "q_teps_0 = model.transition(t_eps * torch.ones((B,), device=device)) # (N, S, S)\n",
    "print(q_teps_0)\n",
    "q_teps_0 = utils.expand_dims(q_teps_0, axis=list(range(1, xt.ndim)))\n",
    "\n",
    "\n",
    "q_t_teps = model.transit_between(t_eps * torch.ones((B,), device=device), t * torch.ones((B,), device=device))  # (N, S, S\n",
    "print(q_t_teps)\n",
    "q_t_teps = q_t_teps.permute(0, 2, 1)\n",
    "b = utils.expand_dims(torch.arange(xt.shape[0]), axis=list(range(1, xt.ndim)))\n",
    "q_t_teps = q_t_teps[b, xt.long()].unsqueeze(-2)\n",
    "print(\"q_teps_0\", q_teps_0, q_teps_0.shape)\n",
    "print(\"q_t_teps\", q_t_teps, q_t_teps.shape)\n",
    "start_opt = time.time()\n",
    "qt0 = q_teps_0 * q_t_teps \n",
    "print(\"qt0\", qt0, qt0.shape)\n",
    "\n",
    "end_opt = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((qt0 >= 0).all())\n",
    "print(torch.log(qt0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.where(q_teps_0 <= 0.0, -1e9, torch.log(q_teps_0))\n",
    "b = torch.where(q_t_teps <= 0.0, -1e9, torch.log(q_t_teps))\n",
    "c = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = utils.expand_dims(torch.arange(xt.shape[0]), axis=list(range(1, xt.ndim)))\n",
    "q_t_teps = q_t_teps[b, xt.long()].unsqueeze(-2)\n",
    "qt0 = q_teps_0 * q_t_teps # 30-60sekunden\n",
    "\n",
    "log_qt0 = torch.where(qt0 <= 0.0, -1e9, torch.log(qt0)) # 7min\n",
    "start_opt = time.time()\n",
    "log_p0t = log_p0t.unsqueeze(-1)\n",
    "log_prob = torch.logsumexp(log_p0t + log_qt0, dim=-2)\n",
    "end_opt = time.time()\n",
    "print(end_opt - start_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_qt0, log_qt0.shape)\n",
    "print(c, c.shape)\n",
    "print(c == log_qt0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_p0t = log_p0t.unsqueeze(-1)\n",
    "log_prob = torch.logsumexp(log_p0t + log_qt0, dim=-2)\n",
    "# axis kein parameter? fehler hier\n",
    "end_opt = time.time()\n",
    "print(\"sampling operations time\", end_opt - start_opt)\n",
    "q_teps_0 = model.transition(t_eps * torch.ones((B,), device=device)) # (N, S, S)\n",
    "print(q_teps_0, q_teps_0.shape)\n",
    "q_teps_0 = utils.expand_dims(q_teps_0, axis=list(range(1, xt.ndim)))\n",
    "print(q_teps_0, q_teps_0.shape)\n",
    "q_t_teps = model.transit_between(t_eps * torch.ones((B,), device=device), t * torch.ones((B,), device=device))  # (N, S, S\n",
    "print(q_t_teps, q_t_teps.shape)\n",
    "q_t_teps = q_t_teps.permute(0, 2, 1)\n",
    "print(q_t_teps, q_t_teps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = utils.expand_dims(torch.arange(xt.shape[0]), axis=list(range(1, xt.ndim)))\n",
    "print(b, b.shape)\n",
    "q_t_teps = q_t_teps[b, xt].unsqueeze(-2)\n",
    "print(q_t_teps, q_t_teps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Transition matrix q_t|0: x0 -> xt ---------------------\n",
    "cfg = get_config()\n",
    "cfg.data.S = S\n",
    "cfg.model.rate_const = rate_const\n",
    "uni = UniformRate(cfg, 'cpu')\n",
    "ts = torch.rand((B,))\n",
    "qt0 = uni.transition(ts)\n",
    "x0= torch.randint(low=0, high=S, size=(B, D), dtype=torch.int)\n",
    "#print(x0)\n",
    "#print(qt0, qt0.shape)\n",
    "qt0_rows_reg = qt0[\n",
    "    torch.arange(B, device=device).repeat_interleave(\n",
    "        D\n",
    "    ),  # repeats every element 0 to B-1 D-times\n",
    "    x0.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "    :,\n",
    "]\n",
    "print(qt0_rows_reg, qt0_rows_reg.shape)\n",
    "b = utils.expand_dims(torch.arange(B), (tuple(range(1, x0.dim()))))\n",
    "qt0_rows_reg2 = qt0[b, x0] #.view(-1, S)\n",
    "\n",
    "logits = torch.where(qt0 <= 0.0, -1e9, torch.log(qt0_rows_reg2))\n",
    "\n",
    "\n",
    "x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "x_t = x_t_cat.sample().view(B, D)\n",
    "print(x_t, x_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------- Transition rate: x_t -> x_tilde ------------------\n",
    "rate = uni.rate(ts)\n",
    "#print(rate, rate.shape) # B, S, S\n",
    "rate_vals_square = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "    ]\n",
    "#print(rate_vals_square, rate_vals_square.shape)\n",
    "rate_vals_square[\n",
    "        torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "    ] = 0.0 \n",
    "print(rate_vals_square, rate_vals_square.shape)\n",
    "\n",
    "rate_vals_square = rate_vals_square.view(B, D, S)\n",
    "print(rate_vals_square, rate_vals_square.shape)\n",
    "\n",
    "rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(B, D)\n",
    "print(rate_vals_square_dimsum, rate_vals_square_dimsum.shape)\n",
    "\n",
    "square_dimcat = torch.distributions.categorical.Categorical(rate_vals_square_dimsum)\n",
    "\n",
    "square_dims = square_dimcat.sample() # sampled where transition takes places in every row of B\n",
    "print(\"Where transition\", square_dims, square_dims.shape)\n",
    "\n",
    "rate_new_val_probs = rate_vals_square[\n",
    "    torch.arange(B, device=device), square_dims, :\n",
    "]  # (B, S)\n",
    "print(rate_new_val_probs, rate_new_val_probs.shape)\n",
    "\n",
    "square_newvalcat = torch.distributions.categorical.Categorical(\n",
    "    rate_new_val_probs\n",
    ")\n",
    "\n",
    "# Shape: (B,) mit Werten im Bereich [0, S)\n",
    "square_newval_samples = (\n",
    "    square_newvalcat.sample()\n",
    ")\n",
    "print(\"Transition value\", square_newval_samples, square_newval_samples.shape)\n",
    "\n",
    "x_tilde = x_t.clone()\n",
    "        # noisy image \n",
    "x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "print(x_t)\n",
    "print(x_tilde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------ELBO-------------------\n",
    "mask_reg = torch.ones((B, D, S), device=device)\n",
    "\n",
    "mask_reg[\n",
    "    torch.arange(B, device=device).repeat_interleave(D),\n",
    "    torch.arange(D, device=device).repeat(B),\n",
    "    x_tilde.long().flatten(),\n",
    "] = 0.0\n",
    "print(x_tilde)\n",
    "print(mask_reg, mask_reg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt0_numer_reg = qt0.view(B, S, S)\n",
    "print(qt0_numer_reg , qt0_numer_reg.shape)\n",
    "# q_{t|0} (x|x_0)\n",
    "qt0_denom_reg = (\n",
    "    qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(D),\n",
    "        :,\n",
    "        x_tilde.long().flatten(),\n",
    "    ].view(B, D, S)\n",
    "    + 1e-6\n",
    ")\n",
    "#print(qt0_denom_reg, qt0_denom_reg.shape)\n",
    "\n",
    "#print(rate, rate.shape)\n",
    "rate_vals_reg = rate[\n",
    "    torch.arange(B, device=device).repeat_interleave(D),\n",
    "    :,\n",
    "    x_tilde.long().flatten(),\n",
    "].view(B, D, S)\n",
    "print(rate_vals_reg, rate_vals_reg.shape)\n",
    "print((mask_reg * rate_vals_reg))\n",
    "reg_tmp = (mask_reg * rate_vals_reg) @ qt0_numer_reg.transpose(1, 2)\n",
    "print(reg_tmp, reg_tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_const = 1\n",
    "S = 3\n",
    "B = 2\n",
    "D = 4\n",
    "cfg = get_config()\n",
    "cfg.data.S = S\n",
    "cfg.model.rate_const = rate_const\n",
    "uni = UniformRate(cfg, 'cpu')\n",
    "ts = torch.rand((B,))\n",
    "xt= torch.randint(low=0, high=S, size=(B, D), dtype=torch.int)\n",
    "print(xt)\n",
    "\n",
    "qt0 = uni.transition(ts)\n",
    "\n",
    "qt0_y2x = qt0.permute(0, 2, 1)\n",
    "print(qt0, qt0.shape)\n",
    "print(qt0_y2x, qt0_y2x.shape)\n",
    "print(qt0 == qt0_y2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = utils.expand_dims(\n",
    "    torch.arange(xt.shape[0]), tuple(range(1, xt.dim()))\n",
    ")\n",
    "print(b, b.shape)\n",
    "qt0_y2x = qt0_y2x[b, xt]\n",
    "print(qt0_y2x, qt0_y2x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = qt0_y2x\n",
    "log_p0t = F.log_softmax(logits, dim=-1)\n",
    "print(log_p0t, log_p0t.shape)\n",
    "log_qt0 = torch.where(qt0 <= 1e-35, -1e9, torch.log(qt0))\n",
    "print(log_qt0, log_qt0.shape)\n",
    "log_qt0 = utils.expand_dims(log_qt0, axis=list(range(1, xt.dim())))\n",
    "print(log_qt0, log_qt0.shape)\n",
    "log_p0t = log_p0t.unsqueeze(-1)\n",
    "print(log_p0t, log_p0t.shape)\n",
    "log_prob = torch.logsumexp(log_p0t + log_qt0, dim=-2)\n",
    "print(log_prob, log_prob.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_onehot = F.one_hot(xt.long(), S)\n",
    "qt0 = uni.transition(ts)\n",
    "p0t = F.softmax(logits, dim=-1)\n",
    "print(p0t, p0t.shape)\n",
    "qt0 = utils.expand_dims(qt0, axis=list(range(1, xt.dim() - 1)))\n",
    "print(qt0, qt0.shape)\n",
    "prob_all = p0t @ qt0\n",
    "print(prob_all.shape)\n",
    "log_prob = torch.log(prob_all + 1e-35)\n",
    "print(log_prob, log_prob.shape)\n",
    "log_xt = torch.sum(log_prob * xt_onehot, axis=-1)\n",
    "print(log_xt, log_xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt0 = uni.transition(ts)\n",
    "t_eps = ts - 0.1\n",
    "q_t_teps = uni.transit_between(t_eps * torch.ones((B,), device=device), ts * torch.ones((B,), device=device))\n",
    "print(q_t_teps, q_t_teps.shape, qt0.shape)\n",
    "b = utils.expand_dims(torch.arange(B), (tuple(range(1, x0.dim()))))\n",
    "qt0_rows_reg2 = qt0[b, x0]\n",
    "print(qt0_rows_reg2, qt0_rows_reg2.shape)\n",
    "logits = torch.where(qt0_rows_reg2  <= 0.0, -1e9, torch.log(qt0_rows_reg2))\n",
    "print(logits, logits.shape)\n",
    "\n",
    "x_t_cat = torch.distributions.categorical.Categorical(logits).sample()\n",
    "print(x_t_cat,x_t_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_xt = xt #B, D\n",
    "ll_all =  logits# B, D, S\n",
    "loss = -(\n",
    "    (S - 1) * ll_xt\n",
    "    + torch.sum(utils.log1mexp(ll_all), dim=-1)\n",
    "    - utils.log1mexp(ll_xt)\n",
    ")\n",
    "print(loss, loss.shape)\n",
    "weight = torch.ones((B,), dtype=torch.float32)\n",
    "weight = utils.expand_dims(weight, axis=list(range(1, loss.dim())))\n",
    "print(weight, weight.shape)\n",
    "loss = loss * weight\n",
    "print(loss, loss.shape)\n",
    "loss = torch.sum(loss) / xt.shape[0]\n",
    "print(loss, loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_xt = xt #B, D\n",
    "ll_all =  logits\n",
    "xt_onehot = F.one_hot(xt.long(), num_classes=S)\n",
    "b = utils.expand_dims(torch.arange(xt.shape[0]), tuple(range(1, xt.dim())))\n",
    "print(b, b.shape)\n",
    "qt0_x2y = uni.transition(ts)\n",
    "print(qt0_x2y, qt0_x2y.shape)\n",
    "qt0_y2x = qt0_x2y.permute(0, 2, 1)\n",
    "print(qt0_x2y, qt0_x2y.shape)\n",
    "qt0_y2x = qt0_y2x[b, xt]\n",
    "print(qt0_x2y, qt0_x2y.shape)\n",
    "ll_xt = ll_xt.unsqueeze(-1)\n",
    "print(\"ll\", ll_xt, ll_xt.shape)\n",
    "backwd = torch.exp(ll_all - ll_xt) * qt0_y2x\n",
    "print(backwd , backwd.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_term = torch.sum(backwd * (1 - xt_onehot), dim=-1)\n",
    "print(first_term , first_term.shape)\n",
    "qt0_x2y = qt0_x2y[b, xt]\n",
    "print(qt0_x2y, qt0_x2y.shape)\n",
    "fwd = (ll_xt - ll_all) * qt0_x2y\n",
    "print(fwd, fwd.shape)\n",
    "second_term = torch.sum(fwd * (1 - xt_onehot), dim=-1)\n",
    "print(second_term, second_term.shape)\n",
    "loss = first_term - second_term\n",
    "print(loss, loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.ones((B,), dtype=torch.float32)\n",
    "weight = utils.expand_dims(weight, axis=list(range(1, loss.dim())))\n",
    "print(weight, weight.shape)\n",
    "loss = loss * weight\n",
    "print(loss, loss.shape)\n",
    "loss = torch.sum(loss) / xt.shape[0]\n",
    "print(loss, loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.concatenate((np.linspace(1.0, 1e-3, 1000), np.array([0])))\n",
    "#save_ts = ts[np.linspace(0, len(ts)-2, num_intermediates, dtype=int)]\n",
    "\n",
    "for idx, t in (enumerate(ts[0:-1])):\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
