{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from lib.models.models import UniformRate, UniformVariantRate\n",
    "import ml_collections\n",
    "from config.config_hollow import get_config\n",
    "from lib.utils import utils\n",
    "import torch.nn.functional as F\n",
    "from lib.networks.networks_paul import UNet\n",
    "from lib.networks.hollow import BidirectionalTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "from lib.datasets.datasets import (\n",
    "    create_train_discrete_mnist_dataloader,\n",
    "    get_binmnist_datasets,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "Now, when we minimize LCT, we are sampling (x, x ̃) from the forward process and then maximizing the assigned model probability for \n",
    "\n",
    "the pairing in the reverse direction, just as in LDT. The slight extra complexity comes from the fact we areconsidering the case \n",
    "\n",
    "when xk = xk+1 and the case when xk ̸= xk+1 separately. When xk = xk+1, this corresponds to the first term in LCT which we can see \n",
    "\n",
    "is minimizing the reverse rate out of x which is exactly maximizing the model probability for no transition to occur. When xk ̸= xk+1, \n",
    "\n",
    "this corresponds to the second term in LCT, which is maximizing the reverse rate from x ̃ to x which in turn maximizes the model probability \n",
    "\n",
    "for the x ̃ to x transition to occur.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1 * torch.ones((4, ))\n",
    "cosi = -torch.sqrt(torch.cos(torch.pi / 2 * t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg =  get_config()\n",
    "cfg.model.t_func = \"log_sqr\"\n",
    "cfg.model.rate_const = 0.5\n",
    "dataloader = create_train_discrete_mnist_dataloader(\n",
    "        batch_size=cfg.data.batch_size, image_size=cfg.data.image_size, use_augmentation=cfg.data.use_augm\n",
    "    )\n",
    "#train_set, _, _ = get_binmnist_datasets('/Users/paulheller/PythonRepositories/Master-Thesis/ContTimeDiscreteSpace/TAUnSDDM/lib/datasets/', device=\"cpu\")\n",
    "#dataloader = DataLoader(train_set, batch_size=cfg.data.batch_size, shuffle=True, num_workers=4)\n",
    "model = UniformRate(cfg, 'cpu')\n",
    "#model = UniformVariantRate(cfg, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist_batch(batch, save_path=None):\n",
    "    \"\"\"Plottet ein Batch von MNIST-Bildern.\"\"\"\n",
    "    num_images = batch.shape[0]\n",
    "    sqrt_num_images = int(np.sqrt(num_images))\n",
    "\n",
    "    fig, axes = plt.subplots(sqrt_num_images, sqrt_num_images, figsize=(8, 8))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = batch[i].squeeze()\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "        ax.axis(\"off\") \n",
    "\n",
    "    plt.tight_layout() \n",
    "    #plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor(129.0727)\n",
      "Std: tensor(75.6747)\n",
      "Mean: tensor(128.1875)\n",
      "Std: tensor(74.2657)\n",
      "Mean: tensor(127.6195)\n",
      "Std: tensor(74.2886)\n",
      "Mean: tensor(128.3256)\n",
      "Std: tensor(73.8019)\n",
      "Mean: tensor(128.7630)\n",
      "Std: tensor(74.3003)\n",
      "Mean: tensor(128.2710)\n",
      "Std: tensor(74.1842)\n",
      "Mean: tensor(128.0754)\n",
      "Std: tensor(74.2722)\n",
      "Mean: tensor(127.5572)\n",
      "Std: tensor(73.9721)\n",
      "Mean: tensor(127.8145)\n",
      "Std: tensor(73.9156)\n",
      "Mean: tensor(127.5807)\n",
      "Std: tensor(73.9485)\n",
      "Mean: tensor(127.3871)\n",
      "Std: tensor(73.9578)\n",
      "Mean: tensor(127.5563)\n",
      "Std: tensor(73.8246)\n",
      "Mean: tensor(127.1470)\n",
      "Std: tensor(73.8401)\n",
      "Mean: tensor(127.4376)\n",
      "Std: tensor(73.8070)\n",
      "Mean: tensor(127.3521)\n",
      "Std: tensor(73.8558)\n",
      "Mean: tensor(127.3603)\n",
      "Std: tensor(73.7548)\n",
      "Mean: tensor(127.4995)\n",
      "Std: tensor(73.8081)\n",
      "Mean: tensor(127.3558)\n",
      "Std: tensor(73.7213)\n",
      "Mean: tensor(127.2624)\n",
      "Std: tensor(73.7455)\n",
      "Mean: tensor(127.2756)\n",
      "Std: tensor(73.7511)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m device \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mdevice\n\u001b[1;32m     11\u001b[0m ts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand((B,), device\u001b[39m=\u001b[39mdevice) \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m \u001b[39m0.01\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m0.01\u001b[39m\n\u001b[0;32m---> 13\u001b[0m qt0 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtransition(ts)  \u001b[39m# (B, S, S)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# rate = model.rate(ts)  # (B, S, S)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m b \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mexpand_dims(torch\u001b[39m.\u001b[39marange(B), (\u001b[39mtuple\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, minibatch\u001b[39m.\u001b[39mdim()))))\n",
      "File \u001b[0;32m~/PythonRepositories/Master-Thesis/ContTimeDiscreteSpace/TAUnSDDM/lib/models/models.py:325\u001b[0m, in \u001b[0;36mUniformRate.transition\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    316\u001b[0m S \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mS\n\u001b[1;32m    317\u001b[0m transitions \u001b[39m=\u001b[39m (\n\u001b[1;32m    318\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meigvecs\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, S, S)  \u001b[39m# Q\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39m@\u001b[39m torch\u001b[39m.\u001b[39mdiag_embed(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meigvecs\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, S, S)  \u001b[39m# Q^-1\u001b[39;00m\n\u001b[1;32m    323\u001b[0m )\n\u001b[0;32m--> 325\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39;49mmin(transitions) \u001b[39m<\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1e-6\u001b[39m:\n\u001b[1;32m    326\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    327\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Warning] UniformRate, large negative transition values \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mmin(transitions)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m transitions[transitions \u001b[39m<\u001b[39m \u001b[39m1e-8\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "x_mean = 0 # 127.5\n",
    "x_std = 0 # 73.7\n",
    "\n",
    "for minibatch, _ in dataloader:\n",
    "    \n",
    "    B, C, H, W = minibatch.shape\n",
    "    minibatch = minibatch.view(B, C * H * W) \n",
    "    # hollow xt, t, l_all, l_xt geht rein\n",
    "    device = cfg.device\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "\n",
    "    qt0 = model.transition(ts)  # (B, S, S)\n",
    "\n",
    "    # rate = model.rate(ts)  # (B, S, S)\n",
    "\n",
    "    b = utils.expand_dims(torch.arange(B), (tuple(range(1, minibatch.dim()))))\n",
    "    qt0 = qt0[b, minibatch.long()]\n",
    "\n",
    "    # log loss\n",
    "    log_qt0 = torch.where(qt0 <= 0.0, -1e9, torch.log(qt0))\n",
    "    xt = torch.distributions.categorical.Categorical(logits=log_qt0).sample()\n",
    "    #print(type(xt))\n",
    "\n",
    "    #print(torch.mean(xt[0,:].float()))\n",
    "    x_mean +=torch.mean(xt[0,:].float())\n",
    "    x_std += torch.std(xt[0,:].float())\n",
    "    i = i+1\n",
    "    print(\"Mean:\", x_mean / i)\n",
    "    print(\"Std:\", x_std / i)\n",
    "    if i == 300:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall tensor(129.9031)\n",
      "overall tensor(129.8731)\n",
      "overall tensor(129.4154)\n",
      "overall tensor(127.7149)\n",
      "overall tensor(127.5316)\n",
      "overall tensor(127.7532)\n",
      "overall tensor(127.4016)\n",
      "overall tensor(127.5756)\n",
      "overall tensor(128.0835)\n",
      "overall tensor(128.1330)\n",
      "overall tensor(127.8340)\n",
      "overall tensor(127.8485)\n",
      "overall tensor(127.5180)\n",
      "overall tensor(127.5259)\n",
      "overall tensor(127.7029)\n",
      "overall tensor(127.6688)\n",
      "overall tensor(127.5735)\n",
      "overall tensor(127.5109)\n",
      "overall tensor(127.5726)\n",
      "overall tensor(127.6734)\n",
      "overall tensor(127.7119)\n",
      "overall tensor(127.7330)\n",
      "overall tensor(127.8131)\n",
      "overall tensor(127.9375)\n",
      "overall tensor(127.6839)\n",
      "overall tensor(127.6849)\n",
      "overall tensor(127.8428)\n",
      "overall tensor(127.6692)\n",
      "overall tensor(127.8840)\n",
      "overall tensor(127.9300)\n",
      "overall tensor(127.8764)\n",
      "overall tensor(127.9904)\n",
      "overall tensor(128.0751)\n",
      "overall tensor(128.0299)\n",
      "overall tensor(128.0891)\n",
      "overall tensor(128.1724)\n",
      "overall tensor(128.0675)\n",
      "overall tensor(128.0328)\n",
      "overall tensor(128.0071)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[39m# get random timestep between 1.0 and self.min_time\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     ts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand((B,), device\u001b[39m=\u001b[39mdevice) \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m \u001b[39m0.01\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m0.01\u001b[39m\n\u001b[0;32m---> 14\u001b[0m     qt0 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtransition(\n\u001b[1;32m     15\u001b[0m ts\n\u001b[1;32m     16\u001b[0m     )  \u001b[39m# (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[39m# R_t = beta_t * R_b\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     rate \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mrate(\n\u001b[1;32m     20\u001b[0m ts\n\u001b[1;32m     21\u001b[0m     )  \u001b[39m# (B, S, S) # no proability in here (diagonal = - sum of rows)\u001b[39;00m\n",
      "File \u001b[0;32m~/PythonRepositories/Master-Thesis/ContTimeDiscreteSpace/TAUnSDDM/lib/models/models.py:325\u001b[0m, in \u001b[0;36mUniformRate.transition\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    316\u001b[0m S \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mS\n\u001b[1;32m    317\u001b[0m transitions \u001b[39m=\u001b[39m (\n\u001b[1;32m    318\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meigvecs\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, S, S)  \u001b[39m# Q\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39m@\u001b[39m torch\u001b[39m.\u001b[39mdiag_embed(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meigvecs\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, S, S)  \u001b[39m# Q^-1\u001b[39;00m\n\u001b[1;32m    323\u001b[0m )\n\u001b[0;32m--> 325\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39;49mmin(transitions) \u001b[39m<\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1e-6\u001b[39m:\n\u001b[1;32m    326\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    327\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Warning] UniformRate, large negative transition values \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mmin(transitions)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m transitions[transitions \u001b[39m<\u001b[39m \u001b[39m1e-8\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "        # if 4 Dim => like images: True\n",
    "i = 0\n",
    "x_mean_t = 0\n",
    "for minibatch, _ in dataloader:\n",
    "    B, C, H, W = minibatch.shape\n",
    "    minibatch = minibatch.view(B, C * H * W)\n",
    "\n",
    "    B, D = minibatch.shape\n",
    "    device = model.device\n",
    "\n",
    "    # get random timestep between 1.0 and self.min_time\n",
    "    ts = torch.rand((B,), device=device) * (1.0 - 0.01) + 0.01\n",
    "\n",
    "    qt0 = model.transition(\n",
    "        ts\n",
    "    )  # (B, S, S) # transition q_{t | s=0} eq.15 => here randomness because of ts => for every ts another q_{t|0}\n",
    "\n",
    "    # R_t = beta_t * R_b\n",
    "    rate = model.rate(\n",
    "        ts\n",
    "    )  # (B, S, S) # no proability in here (diagonal = - sum of rows)\n",
    "\n",
    "    # --------------- Sampling x_t, x_tilde --------------------\n",
    "    # qt0_rows_reg = (B * D, S) probability distribution\n",
    "    # diagonal elements of qt0 (higher probability) will be put at column of value of x_t\n",
    "    # we do this because then we sample from qt0_rows_reg and then it is most likely more similar to x0=batch\n",
    "    # example: q_t0 =   [0.4079, 0.2961, 0.2961],\n",
    "    #                   [0.2961, 0.4079, 0.2961],\n",
    "    #                   [0.2961, 0.2961, 0.4079]],\n",
    "    # batch = (2, 0, 1)\n",
    "    # qt0_rows_reg = [0.2961, 0.2961, 0.4079],\n",
    "    #                [0.4079, 0.2961, 0.4079],\n",
    "    #                [0.2961, 0.4079, 0.2961]\n",
    "\n",
    "    qt0_rows_reg = qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(\n",
    "            D\n",
    "        ),  # repeats every element 0 to B-1 D-times\n",
    "        minibatch.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "        :,\n",
    "    ]  # (B*D, S)\n",
    "\n",
    "    # set of (B*D) categorical distributions with probabilities from qt0_rows_reg\n",
    "    x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "    x_t = x_t_cat.sample().view(  # sampling B * D times => from every row of qt0_rows_reg once => then transform it to shape B, D\n",
    "        B, D\n",
    "    )  # (B*D,) mit view => (B, D) Bsp: x_t = (0, 1, 2, 4, 3) (for B =1 )\n",
    "    #print(torch.mean(x_t[0, :].float()))\n",
    "    x_mean_t +=torch.mean(x_t[0,:].float())\n",
    "    i = i+1\n",
    "    print(\"overall\", x_mean_t / i)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "B = 2\n",
    "D = 4\n",
    "S = 3\n",
    "log_prob = torch.rand((B,D, S))\n",
    "print(log_prob)\n",
    "x = torch.randint(low=0, high=3, size=(B, 1), device='cpu')\n",
    "one_hot = F.one_hot(x)\n",
    "print(one_hot,one_hot.shape)\n",
    "mult = log_prob * one_hot\n",
    "print(mult, mult.shape)\n",
    "sum_mult = torch.sum(mult, dim=-1)\n",
    "print(sum_mult, sum_mult.shape)\n",
    "\n",
    "print(torch.sum(sum_mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config =  get_config()\n",
    "bdt = BidirectionalTransformer(config)\n",
    "print(\"number of parameters: \", sum([p.numel() for p in bdt.parameters()]))\n",
    "B = config.data.batch_size\n",
    "D = config.concat_dim\n",
    "S = config.data.S\n",
    "xt= torch.randint(low=0, high=S, size=(B, D), dtype=torch.int)\n",
    "t = 1 * torch.ones((B, ))\n",
    "print(len(t.shape))\n",
    "# x_pred = bdt(xt, t)\n",
    "#print(x_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config =  get_config()\n",
    "model = UniformRate(config, 'cpu')\n",
    "S = 256\n",
    "B = 32\n",
    "D = 1024\n",
    "y = torch.randint(low=0, high=S, size=(B, D), dtype=torch.int)\n",
    "t = 1 * torch.ones((B, ))\n",
    "rate = model.rate_mat(y, t)\n",
    "print(rate.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "S=256\n",
    "rate_const = 1\n",
    "\n",
    "\n",
    "cfg.data.S = S\n",
    "device = 'cpu'\n",
    "cfg.model.rate_const = rate_const\n",
    "\n",
    "S = 256\n",
    "B = 64\n",
    "D = 1024\n",
    "model = UniformRate(cfg, 'cpu')\n",
    "unet = UNet(\n",
    "                in_channel=1,\n",
    "                out_channel=1,\n",
    "                channel=32,\n",
    "                channel_multiplier=cfg.model.ch_mult,\n",
    "                n_res_blocks=cfg.model.num_res_blocks,\n",
    "                attn_resolutions=[16],\n",
    "                num_heads=1,\n",
    "                dropout=cfg.model.dropout,\n",
    "                model_output = 'logits',  # 'logits' or 'logistic_pars'\n",
    "                num_classes=S,\n",
    "                x_min_max=(0, 255),\n",
    "                img_size=32,\n",
    "        )\n",
    "t = 1\n",
    "xt= torch.randint(low=0, high=S, size=(B, D), dtype=torch.int)\n",
    "B, D = xt.shape\n",
    "C, H, W = (1, 32, 32)\n",
    "S = 256\n",
    "x = xt.view(B, C, H, W)\n",
    "x_pred = unet(x, t * torch.ones((B,), device=device))\n",
    "x_pred = x_pred.view(B, D, S)\n",
    "print(\"PRED\",x_pred, x_pred.shape)\n",
    "log_p0t = F.log_softmax(x_pred, dim=2)\n",
    "print(log_p0t, log_p0t.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def transformer_timestep_embedding(timesteps, embedding_dim, max_positions=10000):\n",
    "    assert embedding_dim % 2 == 0\n",
    "    assert len(timesteps.shape) == 1\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(max_positions) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = timesteps[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "x = torch.randint(0, S, (B, D))  # Zufällige Integer zwischen 0 und vocab_size - 1\n",
    "\n",
    "# Embedding-Layer\n",
    "embedding = nn.Embedding(S, 10)\n",
    "x = embedding(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "x = torch.randint(0, S, (B, D))  # Zufällige Integer zwischen 0 und vocab_size - 1\n",
    "\n",
    "# Embedding-Layer\n",
    "embedding = nn.Embedding(S, 10)\n",
    "x = embedding(x)\n",
    "print(x.shape)\n",
    "\n",
    "t =1 * torch.ones((B,))\n",
    "temb = transformer_timestep_embedding(t, 10, max_positions=10000)\n",
    "\n",
    "assert x.ndim == 3 and temb.ndim == 2 # B, D, E and B, E\n",
    "print(temb.shape)\n",
    "temb = temb.unsqueeze(1)\n",
    "print(temb.shape)\n",
    "conditioner = temb\n",
    "\n",
    "#conditioner = torch.cat([conditioner, temb], dim=1) # B, 2D, E\n",
    "print(conditioner.shape)\n",
    "cond_dim = conditioner.size(1) # 2D\n",
    "print(\"cond_dim\", cond_dim)\n",
    "concat_dim = x.size(1) + cond_dim - 1 # 3D -1\n",
    "print(\"conc dim\", concat_dim)\n",
    "pos_idx = torch.arange(concat_dim, dtype=torch.int32).unsqueeze(0)\n",
    "print(\"pos\", pos_idx.shape)\n",
    "print(\"x[:, :-1]\", x[:, :-1].shape)\n",
    "x = torch.cat([conditioner, x[:, :-1]], dim=1)\n",
    "print(\"conditioner\", conditioner.shape)\n",
    "print(\"x\", x.shape)\n",
    "mask = pos_idx.unsqueeze(-1) <= pos_idx.unsqueeze(-2)\n",
    "mask = mask.unsqueeze(-3) \n",
    "print(\"mask\", mask, mask.shape)\n",
    "mask = mask[:, :, -cond_dim:, -cond_dim:] = 1.0\n",
    "print(\"mask\", mask, mask.shape)\n",
    "#attn_mask = torch.triu(torch.ones((concat_dim, concat_dim)), diagonal=1).bool()\n",
    "attn_mask = torch.tril(torch.ones(concat_dim, concat_dim), diagonal=1).bool()\n",
    "print(\"mask\", mask, mask.shape)\n",
    "print(mask.squeeze(0).squeeze(0) == ~attn_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = torch.triu(torch.ones((5, 5)), diagonal=1).bool()\n",
    "print(attn_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = torch.tril(torch.ones(5, 5), diagonal=-1).bool()\n",
    "print(attn_mask)\n",
    "#print(attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt_test = model.transition\n",
    "print(qt_test.shape)\n",
    "qt_test = utils.expand_dims(qt_test, axis=list(range(1, xt.dim() - 1)))\n",
    "print(qt_test.shape)\n",
    "torch.where(qt_test <= 1e-35, -1e9, torch.log(qt_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.01\n",
    "start_opt = time.time()\n",
    "t_eps = t - h #tau\n",
    "q_teps_0 = model.transition(t_eps * torch.ones((B,), device=device)) # (N, S, S)\n",
    "print(q_teps_0)\n",
    "q_teps_0 = utils.expand_dims(q_teps_0, axis=list(range(1, xt.ndim)))\n",
    "\n",
    "\n",
    "q_t_teps = model.transit_between(t_eps * torch.ones((B,), device=device), t * torch.ones((B,), device=device))  # (N, S, S\n",
    "print(q_t_teps)\n",
    "q_t_teps = q_t_teps.permute(0, 2, 1)\n",
    "b = utils.expand_dims(torch.arange(xt.shape[0]), axis=list(range(1, xt.ndim)))\n",
    "q_t_teps = q_t_teps[b, xt.long()].unsqueeze(-2)\n",
    "print(\"q_teps_0\", q_teps_0, q_teps_0.shape)\n",
    "print(\"q_t_teps\", q_t_teps, q_t_teps.shape)\n",
    "start_opt = time.time()\n",
    "qt0 = q_teps_0 * q_t_teps \n",
    "print(\"qt0\", qt0, qt0.shape)\n",
    "\n",
    "end_opt = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((qt0 >= 0).all())\n",
    "print(torch.log(qt0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.where(q_teps_0 <= 0.0, -1e9, torch.log(q_teps_0))\n",
    "b = torch.where(q_t_teps <= 0.0, -1e9, torch.log(q_t_teps))\n",
    "c = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = utils.expand_dims(torch.arange(xt.shape[0]), axis=list(range(1, xt.ndim)))\n",
    "q_t_teps = q_t_teps[b, xt.long()].unsqueeze(-2)\n",
    "qt0 = q_teps_0 * q_t_teps # 30-60sekunden\n",
    "\n",
    "log_qt0 = torch.where(qt0 <= 0.0, -1e9, torch.log(qt0)) # 7min\n",
    "start_opt = time.time()\n",
    "log_p0t = log_p0t.unsqueeze(-1)\n",
    "log_prob = torch.logsumexp(log_p0t + log_qt0, dim=-2)\n",
    "end_opt = time.time()\n",
    "print(end_opt - start_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_qt0, log_qt0.shape)\n",
    "print(c, c.shape)\n",
    "print(c == log_qt0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_p0t = log_p0t.unsqueeze(-1)\n",
    "log_prob = torch.logsumexp(log_p0t + log_qt0, dim=-2)\n",
    "# axis kein parameter? fehler hier\n",
    "end_opt = time.time()\n",
    "print(\"sampling operations time\", end_opt - start_opt)\n",
    "q_teps_0 = model.transition(t_eps * torch.ones((B,), device=device)) # (N, S, S)\n",
    "print(q_teps_0, q_teps_0.shape)\n",
    "q_teps_0 = utils.expand_dims(q_teps_0, axis=list(range(1, xt.ndim)))\n",
    "print(q_teps_0, q_teps_0.shape)\n",
    "q_t_teps = model.transit_between(t_eps * torch.ones((B,), device=device), t * torch.ones((B,), device=device))  # (N, S, S\n",
    "print(q_t_teps, q_t_teps.shape)\n",
    "q_t_teps = q_t_teps.permute(0, 2, 1)\n",
    "print(q_t_teps, q_t_teps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = utils.expand_dims(torch.arange(xt.shape[0]), axis=list(range(1, xt.ndim)))\n",
    "print(b, b.shape)\n",
    "q_t_teps = q_t_teps[b, xt].unsqueeze(-2)\n",
    "print(q_t_teps, q_t_teps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Transition matrix q_t|0: x0 -> xt ---------------------\n",
    "cfg = get_config()\n",
    "cfg.data.S = S\n",
    "cfg.model.rate_const = rate_const\n",
    "uni = UniformRate(cfg, 'cpu')\n",
    "ts = torch.rand((B,))\n",
    "qt0 = uni.transition(ts)\n",
    "x0= torch.randint(low=0, high=S, size=(B, D), dtype=torch.int)\n",
    "#print(x0)\n",
    "#print(qt0, qt0.shape)\n",
    "qt0_rows_reg = qt0[\n",
    "    torch.arange(B, device=device).repeat_interleave(\n",
    "        D\n",
    "    ),  # repeats every element 0 to B-1 D-times\n",
    "    x0.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "    :,\n",
    "]\n",
    "print(qt0_rows_reg, qt0_rows_reg.shape)\n",
    "b = utils.expand_dims(torch.arange(B), (tuple(range(1, x0.dim()))))\n",
    "qt0_rows_reg2 = qt0[b, x0] #.view(-1, S)\n",
    "\n",
    "logits = torch.where(qt0 <= 0.0, -1e9, torch.log(qt0_rows_reg2))\n",
    "\n",
    "\n",
    "x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "x_t = x_t_cat.sample().view(B, D)\n",
    "print(x_t, x_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------- Transition rate: x_t -> x_tilde ------------------\n",
    "rate = uni.rate(ts)\n",
    "#print(rate, rate.shape) # B, S, S\n",
    "rate_vals_square = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "    ]\n",
    "#print(rate_vals_square, rate_vals_square.shape)\n",
    "rate_vals_square[\n",
    "        torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "    ] = 0.0 \n",
    "print(rate_vals_square, rate_vals_square.shape)\n",
    "\n",
    "rate_vals_square = rate_vals_square.view(B, D, S)\n",
    "print(rate_vals_square, rate_vals_square.shape)\n",
    "\n",
    "rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(B, D)\n",
    "print(rate_vals_square_dimsum, rate_vals_square_dimsum.shape)\n",
    "\n",
    "square_dimcat = torch.distributions.categorical.Categorical(rate_vals_square_dimsum)\n",
    "\n",
    "square_dims = square_dimcat.sample() # sampled where transition takes places in every row of B\n",
    "print(\"Where transition\", square_dims, square_dims.shape)\n",
    "\n",
    "rate_new_val_probs = rate_vals_square[\n",
    "    torch.arange(B, device=device), square_dims, :\n",
    "]  # (B, S)\n",
    "print(rate_new_val_probs, rate_new_val_probs.shape)\n",
    "\n",
    "square_newvalcat = torch.distributions.categorical.Categorical(\n",
    "    rate_new_val_probs\n",
    ")\n",
    "\n",
    "# Shape: (B,) mit Werten im Bereich [0, S)\n",
    "square_newval_samples = (\n",
    "    square_newvalcat.sample()\n",
    ")\n",
    "print(\"Transition value\", square_newval_samples, square_newval_samples.shape)\n",
    "\n",
    "x_tilde = x_t.clone()\n",
    "        # noisy image \n",
    "x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "print(x_t)\n",
    "print(x_tilde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------ELBO-------------------\n",
    "mask_reg = torch.ones((B, D, S), device=device)\n",
    "\n",
    "mask_reg[\n",
    "    torch.arange(B, device=device).repeat_interleave(D),\n",
    "    torch.arange(D, device=device).repeat(B),\n",
    "    x_tilde.long().flatten(),\n",
    "] = 0.0\n",
    "print(x_tilde)\n",
    "print(mask_reg, mask_reg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt0_numer_reg = qt0.view(B, S, S)\n",
    "print(qt0_numer_reg , qt0_numer_reg.shape)\n",
    "# q_{t|0} (x|x_0)\n",
    "qt0_denom_reg = (\n",
    "    qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(D),\n",
    "        :,\n",
    "        x_tilde.long().flatten(),\n",
    "    ].view(B, D, S)\n",
    "    + 1e-6\n",
    ")\n",
    "#print(qt0_denom_reg, qt0_denom_reg.shape)\n",
    "\n",
    "#print(rate, rate.shape)\n",
    "rate_vals_reg = rate[\n",
    "    torch.arange(B, device=device).repeat_interleave(D),\n",
    "    :,\n",
    "    x_tilde.long().flatten(),\n",
    "].view(B, D, S)\n",
    "print(rate_vals_reg, rate_vals_reg.shape)\n",
    "print((mask_reg * rate_vals_reg))\n",
    "reg_tmp = (mask_reg * rate_vals_reg) @ qt0_numer_reg.transpose(1, 2)\n",
    "print(reg_tmp, reg_tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_const = 1\n",
    "S = 3\n",
    "B = 2\n",
    "D = 4\n",
    "cfg = get_config()\n",
    "cfg.data.S = S\n",
    "cfg.model.rate_const = rate_const\n",
    "uni = UniformRate(cfg, 'cpu')\n",
    "ts = torch.rand((B,))\n",
    "xt= torch.randint(low=0, high=S, size=(B, D), dtype=torch.int)\n",
    "print(xt)\n",
    "\n",
    "qt0 = uni.transition(ts)\n",
    "\n",
    "qt0_y2x = qt0.permute(0, 2, 1)\n",
    "print(qt0, qt0.shape)\n",
    "print(qt0_y2x, qt0_y2x.shape)\n",
    "print(qt0 == qt0_y2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = utils.expand_dims(\n",
    "    torch.arange(xt.shape[0]), tuple(range(1, xt.dim()))\n",
    ")\n",
    "print(b, b.shape)\n",
    "qt0_y2x = qt0_y2x[b, xt]\n",
    "print(qt0_y2x, qt0_y2x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = qt0_y2x\n",
    "log_p0t = F.log_softmax(logits, dim=-1)\n",
    "print(log_p0t, log_p0t.shape)\n",
    "log_qt0 = torch.where(qt0 <= 1e-35, -1e9, torch.log(qt0))\n",
    "print(log_qt0, log_qt0.shape)\n",
    "log_qt0 = utils.expand_dims(log_qt0, axis=list(range(1, xt.dim())))\n",
    "print(log_qt0, log_qt0.shape)\n",
    "log_p0t = log_p0t.unsqueeze(-1)\n",
    "print(log_p0t, log_p0t.shape)\n",
    "log_prob = torch.logsumexp(log_p0t + log_qt0, dim=-2)\n",
    "print(log_prob, log_prob.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_onehot = F.one_hot(xt.long(), S)\n",
    "qt0 = uni.transition(ts)\n",
    "p0t = F.softmax(logits, dim=-1)\n",
    "print(p0t, p0t.shape)\n",
    "qt0 = utils.expand_dims(qt0, axis=list(range(1, xt.dim() - 1)))\n",
    "print(qt0, qt0.shape)\n",
    "prob_all = p0t @ qt0\n",
    "print(prob_all.shape)\n",
    "log_prob = torch.log(prob_all + 1e-35)\n",
    "print(log_prob, log_prob.shape)\n",
    "log_xt = torch.sum(log_prob * xt_onehot, axis=-1)\n",
    "print(log_xt, log_xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt0 = uni.transition(ts)\n",
    "t_eps = ts - 0.1\n",
    "q_t_teps = uni.transit_between(t_eps * torch.ones((B,), device=device), ts * torch.ones((B,), device=device))\n",
    "print(q_t_teps, q_t_teps.shape, qt0.shape)\n",
    "b = utils.expand_dims(torch.arange(B), (tuple(range(1, x0.dim()))))\n",
    "qt0_rows_reg2 = qt0[b, x0]\n",
    "print(qt0_rows_reg2, qt0_rows_reg2.shape)\n",
    "logits = torch.where(qt0_rows_reg2  <= 0.0, -1e9, torch.log(qt0_rows_reg2))\n",
    "print(logits, logits.shape)\n",
    "\n",
    "x_t_cat = torch.distributions.categorical.Categorical(logits).sample()\n",
    "print(x_t_cat,x_t_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_xt = xt #B, D\n",
    "ll_all =  logits# B, D, S\n",
    "loss = -(\n",
    "    (S - 1) * ll_xt\n",
    "    + torch.sum(utils.log1mexp(ll_all), dim=-1)\n",
    "    - utils.log1mexp(ll_xt)\n",
    ")\n",
    "print(loss, loss.shape)\n",
    "weight = torch.ones((B,), dtype=torch.float32)\n",
    "weight = utils.expand_dims(weight, axis=list(range(1, loss.dim())))\n",
    "print(weight, weight.shape)\n",
    "loss = loss * weight\n",
    "print(loss, loss.shape)\n",
    "loss = torch.sum(loss) / xt.shape[0]\n",
    "print(loss, loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_xt = xt #B, D\n",
    "ll_all =  logits\n",
    "xt_onehot = F.one_hot(xt.long(), num_classes=S)\n",
    "b = utils.expand_dims(torch.arange(xt.shape[0]), tuple(range(1, xt.dim())))\n",
    "print(b, b.shape)\n",
    "qt0_x2y = uni.transition(ts)\n",
    "print(qt0_x2y, qt0_x2y.shape)\n",
    "qt0_y2x = qt0_x2y.permute(0, 2, 1)\n",
    "print(qt0_x2y, qt0_x2y.shape)\n",
    "qt0_y2x = qt0_y2x[b, xt]\n",
    "print(qt0_x2y, qt0_x2y.shape)\n",
    "ll_xt = ll_xt.unsqueeze(-1)\n",
    "print(\"ll\", ll_xt, ll_xt.shape)\n",
    "backwd = torch.exp(ll_all - ll_xt) * qt0_y2x\n",
    "print(backwd , backwd.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_term = torch.sum(backwd * (1 - xt_onehot), dim=-1)\n",
    "print(first_term , first_term.shape)\n",
    "qt0_x2y = qt0_x2y[b, xt]\n",
    "print(qt0_x2y, qt0_x2y.shape)\n",
    "fwd = (ll_xt - ll_all) * qt0_x2y\n",
    "print(fwd, fwd.shape)\n",
    "second_term = torch.sum(fwd * (1 - xt_onehot), dim=-1)\n",
    "print(second_term, second_term.shape)\n",
    "loss = first_term - second_term\n",
    "print(loss, loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.ones((B,), dtype=torch.float32)\n",
    "weight = utils.expand_dims(weight, axis=list(range(1, loss.dim())))\n",
    "print(weight, weight.shape)\n",
    "loss = loss * weight\n",
    "print(loss, loss.shape)\n",
    "loss = torch.sum(loss) / xt.shape[0]\n",
    "print(loss, loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.concatenate((np.linspace(1.0, 1e-3, 1000), np.array([0])))\n",
    "#save_ts = ts[np.linspace(0, len(ts)-2, num_intermediates, dtype=int)]\n",
    "\n",
    "for idx, t in (enumerate(ts[0:-1])):\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
