{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulheller/PythonRepositories/Master-Thesis/diffvenv/lib/python3.10/site-packages/cooltools/lib/numutils.py:652: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def iterative_correction_symmetric(\n",
      "/Users/paulheller/PythonRepositories/Master-Thesis/diffvenv/lib/python3.10/site-packages/cooltools/lib/numutils.py:727: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def iterative_correction_asymmetric(x, max_iter=1000, tol=1e-5, verbose=False):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import lib.utils.bookkeeping as bookkeeping\n",
    "from torchvision.utils import make_grid\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from lib.losses.losses import loss_fn\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from lib.models.networks import MNISTScoreNet\n",
    "import lib.utils.bookkeeping as bookkeeping\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from lib.config.config_bin_mnist import get_config \n",
    "from lib.datasets.datasets import create_train_discrete_mnist_dataloader, create_discrete_mnist_dataloader, get_binmnist_datasets\n",
    "from lib.sampling.sampling import Euler_Maruyama_sampler\n",
    "# Main file which contrains all DDSM logic\n",
    "from lib.models.ddsm import *\n",
    "#from lib.utils.utils import binary_to_onehot\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_resume = False\n",
    "if not train_resume:\n",
    "    config = get_config()\n",
    "    bookkeeping.save_config(config, config.saving.save_location)\n",
    "\n",
    "else:\n",
    "    path = '/Users/paulheller/PythonRepositories/Master-Thesis/ContTimeDiscreteSpace/DDSM/SavedModels/Bin_MNIST/'\n",
    "    date = '2023-09-08'\n",
    "    config_name = 'config_001.yaml'\n",
    "    config_path = os.path.join(path, date, config_name)\n",
    "    config = bookkeeping.load_config(config_path)\n",
    "\n",
    "device = config.device\n",
    "\n",
    "sb = UnitStickBreakingTransform()\n",
    "v_one, v_zero, v_one_loggrad, v_zero_loggrad, timepoints = torch.load(config.loading.diffusion_weights_path)\n",
    "v_one = v_one.cpu()\n",
    "v_zero = v_zero.cpu()\n",
    "v_one_loggrad = v_one_loggrad.cpu()\n",
    "v_zero_loggrad = v_zero_loggrad.cpu()\n",
    "timepoints = timepoints.cpu()\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "alpha = torch.ones(config.data.num_cat - 1).float()\n",
    "beta =  torch.arange(config.data.num_cat - 1, 0, -1).float()\n",
    "\n",
    "\n",
    "if config.use_fast_diff:\n",
    "    diffuser_func = partial(diffusion_fast_flatdirichlet, noise_factory_one=v_one, v_one_loggrad=v_one_loggrad)\n",
    "else: \n",
    "    diffuser_func = partial(diffusion_factory, noise_factory_one=v_one, noise_factory_zero=v_zero, noise_factory_one_loggrad=v_one_loggrad, noise_factory_zero_loggrad=v_zero_loggrad, alpha=alpha, beta=beta, device=config.device)\n",
    "\n",
    "\n",
    "if config.speed_balanced:\n",
    "    s = 2 / (torch.ones(config.data.num_cat - 1, device=config.device) + torch.arange(config.data.num_cat - 1, 0, -1,\n",
    "                                                                                    device=config.device).float())\n",
    "else:\n",
    "    s = torch.ones(config.data.num_cat - 1, device=config.device)\n",
    "\n",
    "\n",
    "train_set, valid_set, test_set = get_binmnist_datasets(config.loading.dataset_path) # torch.Size([64, 1, 28, 28])\n",
    "train_dataloader = DataLoader(train_set, batch_size=config.data.batch_size, shuffle=True, num_workers=4)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=config.data.batch_size, shuffle=False, num_workers=4)\n",
    "valid_dataloader = None\n",
    "test_dataloader = DataLoader(test_set, batch_size=config.data.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "time_dependent_weights = torch.load(config.loading.time_dep_weights_path)\n",
    "plt.plot(np.arange(1, config.n_time_steps + 1), time_dependent_weights.cpu())\n",
    "plt.title(\"Time Dependent Weights\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters:  1619586\n"
     ]
    }
   ],
   "source": [
    "model = MNISTScoreNet(ch=config.model.ch, ch_mult=config.model.ch_mult, attn=config.model.attn, num_res_blocks=config.model.num_res_blocks, dropout=0.1, time_dependent_weights=time_dependent_weights)\n",
    "print(\"number of parameters: \", sum([p.numel() for p in model.parameters()]))\n",
    "optimizer = Adam(model.parameters(), lr=config.optimizer.lr, weight_decay=config.optimizer.weight_decay)\n",
    "n_iter = 0\n",
    "state = {\"model\": model, \"optimizer\": optimizer, \"n_iter\": 0}\n",
    "\n",
    "if train_resume:\n",
    "    checkpoint_path = config.saving.checkpoint_path\n",
    "    model_name = 'model_32999.pt'\n",
    "    checkpoint_path = os.path.join(path, date, model_name)\n",
    "    state = bookkeeping.load_state(state, checkpoint_path)\n",
    "    config.training.n_iter = 36000\n",
    "    config.sampler.sample_freq = 36000\n",
    "    config.saving.checkpoint_freq = 1000\n",
    "    \n",
    "sampler = Euler_Maruyama_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/938 [00:00<?, ?it/s]/Users/paulheller/PythonRepositories/Master-Thesis/diffvenv/lib/python3.10/site-packages/cooltools/lib/numutils.py:652: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def iterative_correction_symmetric(\n",
      "/Users/paulheller/PythonRepositories/Master-Thesis/diffvenv/lib/python3.10/site-packages/cooltools/lib/numutils.py:652: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def iterative_correction_symmetric(\n",
      "/Users/paulheller/PythonRepositories/Master-Thesis/diffvenv/lib/python3.10/site-packages/cooltools/lib/numutils.py:652: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def iterative_correction_symmetric(\n",
      "/Users/paulheller/PythonRepositories/Master-Thesis/diffvenv/lib/python3.10/site-packages/cooltools/lib/numutils.py:727: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def iterative_correction_asymmetric(x, max_iter=1000, tol=1e-5, verbose=False):\n",
      "/Users/paulheller/PythonRepositories/Master-Thesis/diffvenv/lib/python3.10/site-packages/cooltools/lib/numutils.py:727: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def iterative_correction_asymmetric(x, max_iter=1000, tol=1e-5, verbose=False):\n",
      "/Users/paulheller/PythonRepositories/Master-Thesis/diffvenv/lib/python3.10/site-packages/cooltools/lib/numutils.py:727: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def iterative_correction_asymmetric(x, max_iter=1000, tol=1e-5, verbose=False):\n",
      "/Users/paulheller/PythonRepositories/Master-Thesis/diffvenv/lib/python3.10/site-packages/cooltools/lib/numutils.py:652: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def iterative_correction_symmetric(\n",
      "/Users/paulheller/PythonRepositories/Master-Thesis/diffvenv/lib/python3.10/site-packages/cooltools/lib/numutils.py:727: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def iterative_correction_asymmetric(x, max_iter=1000, tol=1e-5, verbose=False):\n",
      "Training:   1%|          | 9/938 [01:13<1:46:33,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 100/100 [00:55<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples before tensor([[[[8.8722e-01, 1.1278e-01],\n",
      "          [9.5687e-01, 4.3125e-02],\n",
      "          [3.5610e-01, 6.4390e-01],\n",
      "          ...,\n",
      "          [6.0482e-01, 3.9518e-01],\n",
      "          [7.5754e-01, 2.4246e-01],\n",
      "          [5.4586e-01, 4.5414e-01]],\n",
      "\n",
      "         [[2.2288e-01, 7.7712e-01],\n",
      "          [8.9637e-01, 1.0363e-01],\n",
      "          [8.3160e-01, 1.6840e-01],\n",
      "          ...,\n",
      "          [9.8004e-01, 1.9956e-02],\n",
      "          [4.7542e-01, 5.2458e-01],\n",
      "          [4.8738e-02, 9.5126e-01]],\n",
      "\n",
      "         [[5.9091e-01, 4.0909e-01],\n",
      "          [9.0302e-01, 9.6984e-02],\n",
      "          [7.7946e-01, 2.2054e-01],\n",
      "          ...,\n",
      "          [9.2849e-01, 7.1514e-02],\n",
      "          [8.9635e-01, 1.0365e-01],\n",
      "          [2.4241e-01, 7.5759e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[9.7179e-01, 2.8211e-02],\n",
      "          [9.8005e-01, 1.9945e-02],\n",
      "          [9.9999e-01, 1.0014e-05],\n",
      "          ...,\n",
      "          [9.9016e-01, 9.8448e-03],\n",
      "          [9.3022e-01, 6.9777e-02],\n",
      "          [8.2796e-01, 1.7204e-01]],\n",
      "\n",
      "         [[9.6485e-01, 3.5146e-02],\n",
      "          [9.8102e-01, 1.8979e-02],\n",
      "          [9.8995e-01, 1.0047e-02],\n",
      "          ...,\n",
      "          [9.8321e-01, 1.6794e-02],\n",
      "          [8.6141e-01, 1.3859e-01],\n",
      "          [2.7176e-01, 7.2824e-01]],\n",
      "\n",
      "         [[9.1292e-01, 8.7080e-02],\n",
      "          [6.1089e-01, 3.8911e-01],\n",
      "          [9.7584e-01, 2.4164e-02],\n",
      "          ...,\n",
      "          [9.7714e-01, 2.2859e-02],\n",
      "          [4.9255e-01, 5.0745e-01],\n",
      "          [8.3351e-01, 1.6649e-01]]],\n",
      "\n",
      "\n",
      "        [[[5.8977e-02, 9.4102e-01],\n",
      "          [8.0367e-01, 1.9633e-01],\n",
      "          [4.8807e-01, 5.1193e-01],\n",
      "          ...,\n",
      "          [5.1447e-01, 4.8553e-01],\n",
      "          [1.5849e-01, 8.4151e-01],\n",
      "          [5.2836e-02, 9.4716e-01]],\n",
      "\n",
      "         [[4.6545e-01, 5.3455e-01],\n",
      "          [7.1149e-01, 2.8851e-01],\n",
      "          [6.6735e-01, 3.3265e-01],\n",
      "          ...,\n",
      "          [1.7798e-01, 8.2202e-01],\n",
      "          [5.4260e-01, 4.5740e-01],\n",
      "          [7.3468e-01, 2.6532e-01]],\n",
      "\n",
      "         [[7.5415e-01, 2.4585e-01],\n",
      "          [1.7194e-01, 8.2806e-01],\n",
      "          [4.4536e-01, 5.5464e-01],\n",
      "          ...,\n",
      "          [9.5321e-01, 4.6789e-02],\n",
      "          [8.2866e-01, 1.7134e-01],\n",
      "          [8.9197e-01, 1.0803e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[3.3888e-01, 6.6112e-01],\n",
      "          [9.8024e-01, 1.9757e-02],\n",
      "          [9.9094e-01, 9.0570e-03],\n",
      "          ...,\n",
      "          [9.4510e-01, 5.4899e-02],\n",
      "          [4.8055e-01, 5.1945e-01],\n",
      "          [1.9957e-02, 9.8004e-01]],\n",
      "\n",
      "         [[2.1021e-01, 7.8979e-01],\n",
      "          [9.4044e-01, 5.9557e-02],\n",
      "          [9.8122e-01, 1.8778e-02],\n",
      "          ...,\n",
      "          [4.9706e-01, 5.0294e-01],\n",
      "          [9.1621e-02, 9.0838e-01],\n",
      "          [3.0977e-02, 9.6902e-01]],\n",
      "\n",
      "         [[7.7494e-01, 2.2506e-01],\n",
      "          [1.0745e-01, 8.9255e-01],\n",
      "          [6.9198e-01, 3.0802e-01],\n",
      "          ...,\n",
      "          [8.2680e-02, 9.1732e-01],\n",
      "          [2.8255e-01, 7.1745e-01],\n",
      "          [2.5675e-01, 7.4325e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.9955e-02, 9.8004e-01],\n",
      "          [1.7066e-01, 8.2934e-01],\n",
      "          [7.2191e-02, 9.2781e-01],\n",
      "          ...,\n",
      "          [9.6368e-01, 3.6319e-02],\n",
      "          [9.5817e-01, 4.1827e-02],\n",
      "          [4.1976e-01, 5.8024e-01]],\n",
      "\n",
      "         [[3.0582e-01, 6.9418e-01],\n",
      "          [8.5288e-01, 1.4712e-01],\n",
      "          [9.1870e-01, 8.1297e-02],\n",
      "          ...,\n",
      "          [8.7447e-01, 1.2553e-01],\n",
      "          [7.9073e-01, 2.0927e-01],\n",
      "          [3.8839e-01, 6.1161e-01]],\n",
      "\n",
      "         [[1.2249e-01, 8.7751e-01],\n",
      "          [4.3445e-01, 5.6555e-01],\n",
      "          [9.5702e-01, 4.2983e-02],\n",
      "          ...,\n",
      "          [9.2515e-01, 7.4847e-02],\n",
      "          [9.6688e-01, 3.3120e-02],\n",
      "          [1.4936e-01, 8.5064e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.1646e-02, 9.2835e-01],\n",
      "          [6.0780e-01, 3.9220e-01],\n",
      "          [8.5425e-01, 1.4575e-01],\n",
      "          ...,\n",
      "          [6.5693e-01, 3.4307e-01],\n",
      "          [3.0215e-01, 6.9785e-01],\n",
      "          [4.4231e-02, 9.5577e-01]],\n",
      "\n",
      "         [[8.3612e-01, 1.6388e-01],\n",
      "          [1.2421e-01, 8.7579e-01],\n",
      "          [3.5356e-01, 6.4644e-01],\n",
      "          ...,\n",
      "          [3.8544e-01, 6.1456e-01],\n",
      "          [7.6552e-01, 2.3448e-01],\n",
      "          [3.2687e-02, 9.6731e-01]],\n",
      "\n",
      "         [[2.0543e-02, 9.7946e-01],\n",
      "          [5.2101e-02, 9.4790e-01],\n",
      "          [4.1228e-01, 5.8772e-01],\n",
      "          ...,\n",
      "          [8.6173e-02, 9.1383e-01],\n",
      "          [2.6700e-01, 7.3300e-01],\n",
      "          [8.5576e-02, 9.1442e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[6.7058e-01, 3.2942e-01],\n",
      "          [6.4349e-02, 9.3565e-01],\n",
      "          [7.9990e-02, 9.2001e-01],\n",
      "          ...,\n",
      "          [7.4844e-01, 2.5156e-01],\n",
      "          [4.2759e-01, 5.7241e-01],\n",
      "          [6.1334e-02, 9.3867e-01]],\n",
      "\n",
      "         [[1.8453e-01, 8.1547e-01],\n",
      "          [1.2010e-01, 8.7990e-01],\n",
      "          [7.4633e-01, 2.5367e-01],\n",
      "          ...,\n",
      "          [3.6467e-01, 6.3533e-01],\n",
      "          [2.5922e-01, 7.4078e-01],\n",
      "          [7.8318e-01, 2.1682e-01]],\n",
      "\n",
      "         [[3.8023e-01, 6.1977e-01],\n",
      "          [1.4420e-01, 8.5580e-01],\n",
      "          [9.6639e-01, 3.3609e-02],\n",
      "          ...,\n",
      "          [1.6706e-01, 8.3294e-01],\n",
      "          [5.0488e-01, 4.9512e-01],\n",
      "          [1.8976e-01, 8.1024e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[8.5380e-01, 1.4620e-01],\n",
      "          [9.1538e-01, 8.4618e-02],\n",
      "          [9.6578e-01, 3.4225e-02],\n",
      "          ...,\n",
      "          [6.8147e-01, 3.1853e-01],\n",
      "          [9.7753e-01, 2.2470e-02],\n",
      "          [9.0747e-01, 9.2530e-02]],\n",
      "\n",
      "         [[8.5316e-01, 1.4684e-01],\n",
      "          [9.3057e-01, 6.9427e-02],\n",
      "          [7.2165e-01, 2.7835e-01],\n",
      "          ...,\n",
      "          [9.7729e-01, 2.2708e-02],\n",
      "          [9.5044e-01, 4.9562e-02],\n",
      "          [3.4873e-01, 6.5127e-01]],\n",
      "\n",
      "         [[7.2772e-02, 9.2723e-01],\n",
      "          [5.0433e-01, 4.9567e-01],\n",
      "          [1.7825e-01, 8.2175e-01],\n",
      "          ...,\n",
      "          [8.3584e-01, 1.6416e-01],\n",
      "          [2.5892e-01, 7.4108e-01],\n",
      "          [8.3484e-01, 1.6516e-01]]],\n",
      "\n",
      "\n",
      "        [[[4.2547e-01, 5.7453e-01],\n",
      "          [9.1284e-01, 8.7158e-02],\n",
      "          [9.6443e-01, 3.5573e-02],\n",
      "          ...,\n",
      "          [7.0670e-02, 9.2933e-01],\n",
      "          [3.8917e-01, 6.1083e-01],\n",
      "          [2.9477e-02, 9.7052e-01]],\n",
      "\n",
      "         [[2.5480e-01, 7.4520e-01],\n",
      "          [9.7524e-01, 2.4759e-02],\n",
      "          [9.7922e-01, 2.0778e-02],\n",
      "          ...,\n",
      "          [1.4836e-01, 8.5164e-01],\n",
      "          [2.7120e-01, 7.2880e-01],\n",
      "          [6.0279e-02, 9.3972e-01]],\n",
      "\n",
      "         [[9.6235e-01, 3.7646e-02],\n",
      "          [9.8498e-01, 1.5018e-02],\n",
      "          [9.9999e-01, 1.0014e-05],\n",
      "          ...,\n",
      "          [9.6055e-01, 3.9453e-02],\n",
      "          [7.4065e-02, 9.2594e-01],\n",
      "          [5.2671e-02, 9.4733e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2.2681e-01, 7.7319e-01],\n",
      "          [6.3142e-01, 3.6858e-01],\n",
      "          [9.0225e-01, 9.7748e-02],\n",
      "          ...,\n",
      "          [9.8005e-01, 1.9950e-02],\n",
      "          [8.6881e-02, 9.1312e-01],\n",
      "          [9.1308e-01, 8.6920e-02]],\n",
      "\n",
      "         [[2.9307e-02, 9.7069e-01],\n",
      "          [1.4443e-01, 8.5557e-01],\n",
      "          [1.3652e-01, 8.6348e-01],\n",
      "          ...,\n",
      "          [5.4789e-01, 4.5211e-01],\n",
      "          [4.8012e-01, 5.1988e-01],\n",
      "          [4.4387e-01, 5.5613e-01]],\n",
      "\n",
      "         [[1.8848e-01, 8.1152e-01],\n",
      "          [7.0695e-02, 9.2931e-01],\n",
      "          [3.0966e-01, 6.9034e-01],\n",
      "          ...,\n",
      "          [9.7885e-01, 2.1146e-02],\n",
      "          [7.4193e-01, 2.5807e-01],\n",
      "          [3.9187e-01, 6.0813e-01]]],\n",
      "\n",
      "\n",
      "        [[[7.3246e-01, 2.6754e-01],\n",
      "          [7.5792e-01, 2.4208e-01],\n",
      "          [3.8959e-01, 6.1041e-01],\n",
      "          ...,\n",
      "          [8.5959e-02, 9.1404e-01],\n",
      "          [5.4830e-01, 4.5170e-01],\n",
      "          [1.9956e-02, 9.8004e-01]],\n",
      "\n",
      "         [[7.6322e-01, 2.3678e-01],\n",
      "          [9.7038e-01, 2.9619e-02],\n",
      "          [9.7660e-01, 2.3402e-02],\n",
      "          ...,\n",
      "          [8.2954e-01, 1.7046e-01],\n",
      "          [3.1675e-01, 6.8325e-01],\n",
      "          [4.5497e-01, 5.4503e-01]],\n",
      "\n",
      "         [[9.7594e-01, 2.4055e-02],\n",
      "          [9.1419e-01, 8.5808e-02],\n",
      "          [9.9999e-01, 1.0014e-05],\n",
      "          ...,\n",
      "          [5.9096e-01, 4.0904e-01],\n",
      "          [9.1237e-01, 8.7634e-02],\n",
      "          [8.4943e-01, 1.5057e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[9.2136e-01, 7.8638e-02],\n",
      "          [3.9672e-01, 6.0328e-01],\n",
      "          [8.8294e-01, 1.1706e-01],\n",
      "          ...,\n",
      "          [9.1557e-01, 8.4430e-02],\n",
      "          [7.1936e-01, 2.8064e-01],\n",
      "          [3.0968e-01, 6.9032e-01]],\n",
      "\n",
      "         [[2.4508e-01, 7.5492e-01],\n",
      "          [1.0425e-01, 8.9575e-01],\n",
      "          [6.8502e-01, 3.1498e-01],\n",
      "          ...,\n",
      "          [6.7628e-01, 3.2372e-01],\n",
      "          [8.0048e-01, 1.9952e-01],\n",
      "          [2.1895e-01, 7.8105e-01]],\n",
      "\n",
      "         [[3.3649e-01, 6.6351e-01],\n",
      "          [4.7263e-01, 5.2737e-01],\n",
      "          [3.8983e-01, 6.1017e-01],\n",
      "          ...,\n",
      "          [5.5694e-01, 4.4306e-01],\n",
      "          [2.6859e-01, 7.3141e-01],\n",
      "          [2.1729e-01, 7.8271e-01]]]]) torch.Size([16, 28, 28, 2])\n",
      "samples after tensor([[[[8.8722e-01, 1.1278e-01],\n",
      "          [9.5687e-01, 4.3125e-02],\n",
      "          [3.5610e-01, 6.4390e-01],\n",
      "          ...,\n",
      "          [6.0482e-01, 3.9518e-01],\n",
      "          [7.5754e-01, 2.4246e-01],\n",
      "          [5.4586e-01, 4.5414e-01]],\n",
      "\n",
      "         [[2.2288e-01, 7.7712e-01],\n",
      "          [8.9637e-01, 1.0363e-01],\n",
      "          [8.3160e-01, 1.6840e-01],\n",
      "          ...,\n",
      "          [9.8004e-01, 1.9956e-02],\n",
      "          [4.7542e-01, 5.2458e-01],\n",
      "          [4.8738e-02, 9.5126e-01]],\n",
      "\n",
      "         [[5.9091e-01, 4.0909e-01],\n",
      "          [9.0302e-01, 9.6984e-02],\n",
      "          [7.7946e-01, 2.2054e-01],\n",
      "          ...,\n",
      "          [9.2849e-01, 7.1514e-02],\n",
      "          [8.9635e-01, 1.0365e-01],\n",
      "          [2.4241e-01, 7.5759e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[9.7179e-01, 2.8211e-02],\n",
      "          [9.8005e-01, 1.9945e-02],\n",
      "          [9.9999e-01, 1.0014e-05],\n",
      "          ...,\n",
      "          [9.9016e-01, 9.8448e-03],\n",
      "          [9.3022e-01, 6.9777e-02],\n",
      "          [8.2796e-01, 1.7204e-01]],\n",
      "\n",
      "         [[9.6485e-01, 3.5146e-02],\n",
      "          [9.8102e-01, 1.8979e-02],\n",
      "          [9.8995e-01, 1.0047e-02],\n",
      "          ...,\n",
      "          [9.8321e-01, 1.6794e-02],\n",
      "          [8.6141e-01, 1.3859e-01],\n",
      "          [2.7176e-01, 7.2824e-01]],\n",
      "\n",
      "         [[9.1292e-01, 8.7080e-02],\n",
      "          [6.1089e-01, 3.8911e-01],\n",
      "          [9.7584e-01, 2.4164e-02],\n",
      "          ...,\n",
      "          [9.7714e-01, 2.2859e-02],\n",
      "          [4.9255e-01, 5.0745e-01],\n",
      "          [8.3351e-01, 1.6649e-01]]],\n",
      "\n",
      "\n",
      "        [[[5.8977e-02, 9.4102e-01],\n",
      "          [8.0367e-01, 1.9633e-01],\n",
      "          [4.8807e-01, 5.1193e-01],\n",
      "          ...,\n",
      "          [5.1447e-01, 4.8553e-01],\n",
      "          [1.5849e-01, 8.4151e-01],\n",
      "          [5.2836e-02, 9.4716e-01]],\n",
      "\n",
      "         [[4.6545e-01, 5.3455e-01],\n",
      "          [7.1149e-01, 2.8851e-01],\n",
      "          [6.6735e-01, 3.3265e-01],\n",
      "          ...,\n",
      "          [1.7798e-01, 8.2202e-01],\n",
      "          [5.4260e-01, 4.5740e-01],\n",
      "          [7.3468e-01, 2.6532e-01]],\n",
      "\n",
      "         [[7.5415e-01, 2.4585e-01],\n",
      "          [1.7194e-01, 8.2806e-01],\n",
      "          [4.4536e-01, 5.5464e-01],\n",
      "          ...,\n",
      "          [9.5321e-01, 4.6789e-02],\n",
      "          [8.2866e-01, 1.7134e-01],\n",
      "          [8.9197e-01, 1.0803e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[3.3888e-01, 6.6112e-01],\n",
      "          [9.8024e-01, 1.9757e-02],\n",
      "          [9.9094e-01, 9.0570e-03],\n",
      "          ...,\n",
      "          [9.4510e-01, 5.4899e-02],\n",
      "          [4.8055e-01, 5.1945e-01],\n",
      "          [1.9957e-02, 9.8004e-01]],\n",
      "\n",
      "         [[2.1021e-01, 7.8979e-01],\n",
      "          [9.4044e-01, 5.9557e-02],\n",
      "          [9.8122e-01, 1.8778e-02],\n",
      "          ...,\n",
      "          [4.9706e-01, 5.0294e-01],\n",
      "          [9.1621e-02, 9.0838e-01],\n",
      "          [3.0977e-02, 9.6902e-01]],\n",
      "\n",
      "         [[7.7494e-01, 2.2506e-01],\n",
      "          [1.0745e-01, 8.9255e-01],\n",
      "          [6.9198e-01, 3.0802e-01],\n",
      "          ...,\n",
      "          [8.2680e-02, 9.1732e-01],\n",
      "          [2.8255e-01, 7.1745e-01],\n",
      "          [2.5675e-01, 7.4325e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.9955e-02, 9.8004e-01],\n",
      "          [1.7066e-01, 8.2934e-01],\n",
      "          [7.2191e-02, 9.2781e-01],\n",
      "          ...,\n",
      "          [9.6368e-01, 3.6319e-02],\n",
      "          [9.5817e-01, 4.1827e-02],\n",
      "          [4.1976e-01, 5.8024e-01]],\n",
      "\n",
      "         [[3.0582e-01, 6.9418e-01],\n",
      "          [8.5288e-01, 1.4712e-01],\n",
      "          [9.1870e-01, 8.1297e-02],\n",
      "          ...,\n",
      "          [8.7447e-01, 1.2553e-01],\n",
      "          [7.9073e-01, 2.0927e-01],\n",
      "          [3.8839e-01, 6.1161e-01]],\n",
      "\n",
      "         [[1.2249e-01, 8.7751e-01],\n",
      "          [4.3445e-01, 5.6555e-01],\n",
      "          [9.5702e-01, 4.2983e-02],\n",
      "          ...,\n",
      "          [9.2515e-01, 7.4847e-02],\n",
      "          [9.6688e-01, 3.3120e-02],\n",
      "          [1.4936e-01, 8.5064e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.1646e-02, 9.2835e-01],\n",
      "          [6.0780e-01, 3.9220e-01],\n",
      "          [8.5425e-01, 1.4575e-01],\n",
      "          ...,\n",
      "          [6.5693e-01, 3.4307e-01],\n",
      "          [3.0215e-01, 6.9785e-01],\n",
      "          [4.4231e-02, 9.5577e-01]],\n",
      "\n",
      "         [[8.3612e-01, 1.6388e-01],\n",
      "          [1.2421e-01, 8.7579e-01],\n",
      "          [3.5356e-01, 6.4644e-01],\n",
      "          ...,\n",
      "          [3.8544e-01, 6.1456e-01],\n",
      "          [7.6552e-01, 2.3448e-01],\n",
      "          [3.2687e-02, 9.6731e-01]],\n",
      "\n",
      "         [[2.0543e-02, 9.7946e-01],\n",
      "          [5.2101e-02, 9.4790e-01],\n",
      "          [4.1228e-01, 5.8772e-01],\n",
      "          ...,\n",
      "          [8.6173e-02, 9.1383e-01],\n",
      "          [2.6700e-01, 7.3300e-01],\n",
      "          [8.5576e-02, 9.1442e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[6.7058e-01, 3.2942e-01],\n",
      "          [6.4349e-02, 9.3565e-01],\n",
      "          [7.9990e-02, 9.2001e-01],\n",
      "          ...,\n",
      "          [7.4844e-01, 2.5156e-01],\n",
      "          [4.2759e-01, 5.7241e-01],\n",
      "          [6.1334e-02, 9.3867e-01]],\n",
      "\n",
      "         [[1.8453e-01, 8.1547e-01],\n",
      "          [1.2010e-01, 8.7990e-01],\n",
      "          [7.4633e-01, 2.5367e-01],\n",
      "          ...,\n",
      "          [3.6467e-01, 6.3533e-01],\n",
      "          [2.5922e-01, 7.4078e-01],\n",
      "          [7.8318e-01, 2.1682e-01]],\n",
      "\n",
      "         [[3.8023e-01, 6.1977e-01],\n",
      "          [1.4420e-01, 8.5580e-01],\n",
      "          [9.6639e-01, 3.3609e-02],\n",
      "          ...,\n",
      "          [1.6706e-01, 8.3294e-01],\n",
      "          [5.0488e-01, 4.9512e-01],\n",
      "          [1.8976e-01, 8.1024e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[8.5380e-01, 1.4620e-01],\n",
      "          [9.1538e-01, 8.4618e-02],\n",
      "          [9.6578e-01, 3.4225e-02],\n",
      "          ...,\n",
      "          [6.8147e-01, 3.1853e-01],\n",
      "          [9.7753e-01, 2.2470e-02],\n",
      "          [9.0747e-01, 9.2530e-02]],\n",
      "\n",
      "         [[8.5316e-01, 1.4684e-01],\n",
      "          [9.3057e-01, 6.9427e-02],\n",
      "          [7.2165e-01, 2.7835e-01],\n",
      "          ...,\n",
      "          [9.7729e-01, 2.2708e-02],\n",
      "          [9.5044e-01, 4.9562e-02],\n",
      "          [3.4873e-01, 6.5127e-01]],\n",
      "\n",
      "         [[7.2772e-02, 9.2723e-01],\n",
      "          [5.0433e-01, 4.9567e-01],\n",
      "          [1.7825e-01, 8.2175e-01],\n",
      "          ...,\n",
      "          [8.3584e-01, 1.6416e-01],\n",
      "          [2.5892e-01, 7.4108e-01],\n",
      "          [8.3484e-01, 1.6516e-01]]],\n",
      "\n",
      "\n",
      "        [[[4.2547e-01, 5.7453e-01],\n",
      "          [9.1284e-01, 8.7158e-02],\n",
      "          [9.6443e-01, 3.5573e-02],\n",
      "          ...,\n",
      "          [7.0670e-02, 9.2933e-01],\n",
      "          [3.8917e-01, 6.1083e-01],\n",
      "          [2.9477e-02, 9.7052e-01]],\n",
      "\n",
      "         [[2.5480e-01, 7.4520e-01],\n",
      "          [9.7524e-01, 2.4759e-02],\n",
      "          [9.7922e-01, 2.0778e-02],\n",
      "          ...,\n",
      "          [1.4836e-01, 8.5164e-01],\n",
      "          [2.7120e-01, 7.2880e-01],\n",
      "          [6.0279e-02, 9.3972e-01]],\n",
      "\n",
      "         [[9.6235e-01, 3.7646e-02],\n",
      "          [9.8498e-01, 1.5018e-02],\n",
      "          [9.9999e-01, 1.0014e-05],\n",
      "          ...,\n",
      "          [9.6055e-01, 3.9453e-02],\n",
      "          [7.4065e-02, 9.2594e-01],\n",
      "          [5.2671e-02, 9.4733e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2.2681e-01, 7.7319e-01],\n",
      "          [6.3142e-01, 3.6858e-01],\n",
      "          [9.0225e-01, 9.7748e-02],\n",
      "          ...,\n",
      "          [9.8005e-01, 1.9950e-02],\n",
      "          [8.6881e-02, 9.1312e-01],\n",
      "          [9.1308e-01, 8.6920e-02]],\n",
      "\n",
      "         [[2.9307e-02, 9.7069e-01],\n",
      "          [1.4443e-01, 8.5557e-01],\n",
      "          [1.3652e-01, 8.6348e-01],\n",
      "          ...,\n",
      "          [5.4789e-01, 4.5211e-01],\n",
      "          [4.8012e-01, 5.1988e-01],\n",
      "          [4.4387e-01, 5.5613e-01]],\n",
      "\n",
      "         [[1.8848e-01, 8.1152e-01],\n",
      "          [7.0695e-02, 9.2931e-01],\n",
      "          [3.0966e-01, 6.9034e-01],\n",
      "          ...,\n",
      "          [9.7885e-01, 2.1146e-02],\n",
      "          [7.4193e-01, 2.5807e-01],\n",
      "          [3.9187e-01, 6.0813e-01]]],\n",
      "\n",
      "\n",
      "        [[[7.3246e-01, 2.6754e-01],\n",
      "          [7.5792e-01, 2.4208e-01],\n",
      "          [3.8959e-01, 6.1041e-01],\n",
      "          ...,\n",
      "          [8.5959e-02, 9.1404e-01],\n",
      "          [5.4830e-01, 4.5170e-01],\n",
      "          [1.9956e-02, 9.8004e-01]],\n",
      "\n",
      "         [[7.6322e-01, 2.3678e-01],\n",
      "          [9.7038e-01, 2.9619e-02],\n",
      "          [9.7660e-01, 2.3402e-02],\n",
      "          ...,\n",
      "          [8.2954e-01, 1.7046e-01],\n",
      "          [3.1675e-01, 6.8325e-01],\n",
      "          [4.5497e-01, 5.4503e-01]],\n",
      "\n",
      "         [[9.7594e-01, 2.4055e-02],\n",
      "          [9.1419e-01, 8.5808e-02],\n",
      "          [9.9999e-01, 1.0014e-05],\n",
      "          ...,\n",
      "          [5.9096e-01, 4.0904e-01],\n",
      "          [9.1237e-01, 8.7634e-02],\n",
      "          [8.4943e-01, 1.5057e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[9.2136e-01, 7.8638e-02],\n",
      "          [3.9672e-01, 6.0328e-01],\n",
      "          [8.8294e-01, 1.1706e-01],\n",
      "          ...,\n",
      "          [9.1557e-01, 8.4430e-02],\n",
      "          [7.1936e-01, 2.8064e-01],\n",
      "          [3.0968e-01, 6.9032e-01]],\n",
      "\n",
      "         [[2.4508e-01, 7.5492e-01],\n",
      "          [1.0425e-01, 8.9575e-01],\n",
      "          [6.8502e-01, 3.1498e-01],\n",
      "          ...,\n",
      "          [6.7628e-01, 3.2372e-01],\n",
      "          [8.0048e-01, 1.9952e-01],\n",
      "          [2.1895e-01, 7.8105e-01]],\n",
      "\n",
      "         [[3.3649e-01, 6.6351e-01],\n",
      "          [4.7263e-01, 5.2737e-01],\n",
      "          [3.8983e-01, 6.1017e-01],\n",
      "          ...,\n",
      "          [5.5694e-01, 4.4306e-01],\n",
      "          [2.6859e-01, 7.3141e-01],\n",
      "          [2.1729e-01, 7.8271e-01]]]]) torch.Size([16, 28, 28, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 11/938 [02:34<3:36:49, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss after 1 epoch: 3.6207918326059976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "# epochen umrechnen in n_iter => epoch = 20 and dataset 10000 => n_iter = 20 * 10000\n",
    "    avg_loss = 0.\n",
    "    num_items = 0\n",
    "    exit_flag = False\n",
    "\n",
    "    #stime = time.time()\n",
    "\n",
    "    for x_train in tqdm(train_dataloader, desc='Training'):\n",
    "        #print(\"before:\", x_train.shape)\n",
    "        #print(\"squeeze\", x_train.squeeze().long().shape)\n",
    "        #x_train = binary_to_onehot(x_train.squeeze())\n",
    "        x_train = F.one_hot(x_train.squeeze().long(), num_classes=config.data.num_cat)\n",
    "        #print(\"after\", x_train.shape)\n",
    "\n",
    "        # Optional : there are several options for importance sampling here. it needs to match the loss function\n",
    "        random_t = torch.LongTensor(np.random.choice(np.arange(config.n_time_steps), size=x_train.shape[0],\n",
    "                                                    p=(torch.sqrt(time_dependent_weights) / torch.sqrt(\n",
    "                                                        time_dependent_weights).sum()).cpu().detach().numpy()))\n",
    "        # noise data\n",
    "        \"\"\"\n",
    "        if self.config.random_order:\n",
    "            order = np.random.permutation(np.arange(self.config.data.num_cat))\n",
    "            # perturbed_x, perturbed_x_grad = diffusion_fast_flatdirichlet(x[...,order], random_t, v_one, v_one_loggrad)\n",
    "            #perturbed_x, perturbed_x_grad = diffusion_factory(x[..., order], random_t, v_one, v_zero, v_one_loggrad, v_zero_loggrad, alpha, beta) used diffusion facoty\n",
    "            perturbed_x, perturbed_x_grad = self.diffuser_func(x=x_train[..., order], time_ind=random_t)\n",
    "\n",
    "            perturbed_x = perturbed_x[..., np.argsort(order)]\n",
    "            perturbed_x_grad = perturbed_x_grad[..., np.argsort(order)]\n",
    "        else:\n",
    "        \"\"\"\n",
    "        perturbed_x, perturbed_x_grad = diffuser_func(x=x_train.cpu(), time_ind=random_t) # used this: Flat dirichlet\n",
    "        # perturbed_x, perturbed_x_grad = diffusion_factory(x, random_t, v_one, v_zero, v_one_loggrad, v_zero_loggrad, alpha, beta)\n",
    "\n",
    "        perturbed_x = perturbed_x.to(config.device)\n",
    "        perturbed_x_grad = perturbed_x_grad.to(config.device)\n",
    "        random_timepoints = timepoints[random_t].to(config.device)\n",
    "\n",
    "        random_t = random_t.to(config.device)\n",
    "\n",
    "        # änderung hier kein cat x, s\n",
    "        # predict noise?\n",
    "        score = state['model'](perturbed_x, random_timepoints)\n",
    "\n",
    "        # the loss weighting function may change, there are a few options that we will experiment o\n",
    "        \"\"\"\n",
    "        if self.config.random_order:\n",
    "            order = np.random.permutation(np.arange(self.config.data.num_cat))\n",
    "            perturbed_v = self.sb._inverse(perturbed_x[..., order], prevent_nan=True).detach()\n",
    "            loss = torch.mean(torch.mean(\n",
    "                1 / (torch.sqrt(time_dependent_weights))[random_t][(...,) + (None,) * (x_train.ndim - 1)] * self.s[\n",
    "                    (None,) * (x_train.ndim - 1)] * perturbed_v * (1 - perturbed_v) * (\n",
    "                            gx_to_gv(score[..., order], perturbed_x[..., order], create_graph=True) - gx_to_gv(\n",
    "                        perturbed_x_grad[..., order], perturbed_x[..., order])) ** 2, dim=(1)))\n",
    "        else:\n",
    "        \"\"\"\n",
    "        perturbed_v = sb._inverse(perturbed_x, prevent_nan=True).detach()\n",
    "        loss = torch.mean(torch.mean(\n",
    "            1 / (torch.sqrt(time_dependent_weights))[random_t][(...,) + (None,) * (x_train.ndim - 1)] * s[\n",
    "                (None,) * (x_train.ndim - 1)] * perturbed_v * (1 - perturbed_v) * (\n",
    "                        gx_to_gv(score, perturbed_x, create_graph=True) - gx_to_gv(perturbed_x_grad, perturbed_x)) ** 2, dim=(1)))\n",
    "        #loss = loss_fn(x_train, perturbed_x, perturbed_x_grad, perturbed_v, score, self.s, important_sampling_weights=time_dependent_weights)\n",
    "        \n",
    "        state['optimizer'].zero_grad()\n",
    "        loss.backward()\n",
    "        state['optimizer'].step()\n",
    "        avg_loss += loss.item() * x_train.shape[0]\n",
    "        num_items += x_train.shape[0]\n",
    "        #print(\"Loss:\", loss.item())\n",
    "\n",
    "        if (valid_dataloader is not None) and ((state['n_iter'] + 1) % config.training.validation_freq == 0): # 5 => 5 * n_iter\n",
    "            state['model'].eval()\n",
    "            valid_avg_loss = 0.0\n",
    "            valid_num_items = 0\n",
    "            \n",
    "            #with torch.no_grad():\n",
    "            print(\"Validation:\")\n",
    "            for x_valid in tqdm(valid_dataloader, desc='Validation'):\n",
    "                \n",
    "                x_valid = F.one_hot(x_valid.squeeze().long(), num_classes=config.data.num_cat)\n",
    "                random_t = torch.LongTensor(np.random.choice(np.arange(config.n_time_steps),size=x_valid.shape[0],p=(torch.sqrt(time_dependent_weights) / torch.sqrt(time_dependent_weights).sum()).cpu().detach().numpy())).to(device)\n",
    "                \n",
    "                perturbed_x, perturbed_x_grad = diffuser_func(x_valid, random_t)\n",
    "                # perturbed_x, perturbed_x_grad = diffusion_fast_flatdirichlet(x, random_t, v_one, v_one_loggrad)\n",
    "                \n",
    "                perturbed_x = perturbed_x.to(device)\n",
    "                perturbed_x_grad = perturbed_x_grad.to(device)\n",
    "                random_t = random_t.to(device)\n",
    "                random_timepoints = timepoints[random_t]\n",
    "\n",
    "                score = state['model'](perturbed_x, random_timepoints)            \n",
    "                perturbed_v = sb._inverse(perturbed_x, prevent_nan=True).detach()\n",
    "                val_loss = torch.mean(torch.mean(1 / (torch.sqrt(time_dependent_weights))[random_t][(...,) + (None,) * (x_valid.ndim - 1)] * s[(None,) * (x_valid.ndim - 1)] * perturbed_v * (1 - perturbed_v) * (gx_to_gv(score, perturbed_x, create_graph=True) - gx_to_gv(perturbed_x_grad, perturbed_x)) ** 2, dim=(1)))\n",
    "                #val_loss = loss_fn(x_valid, perturbed_x, perturbed_x_grad, perturbed_v, score, self.s, important_sampling_weights=time_dependent_weights)\n",
    "                valid_avg_loss += val_loss.item() * x_valid.shape[0]\n",
    "                valid_num_items += x_valid.shape[0]\n",
    "\n",
    "            print(\"Average Validation Loss\", valid_avg_loss / valid_num_items)\n",
    "            state['model'].train()\n",
    "\n",
    "        if (state['n_iter'] + 1) % config.sampler.sampler_freq == 0 or state['n_iter'] ==  config.training.n_iter- 1: \n",
    "            state['model'].eval()\n",
    "            print(\"Sampling:\")\n",
    "            samples =sampler(state['model'], config.data.shape, batch_size=config.sampler.n_samples, max_time=4, min_time=0.01, num_steps=100, eps=1e-5, random_order=config.random_order, speed_balanced=config.speed_balanced, device=config.device)\n",
    "            # (28, 28, 2)\n",
    "            ## Sample visualization.\n",
    "            print(\"samples before\", samples, samples.shape)\n",
    "            samples = samples.clamp(0.0, config.data.num_cat)\n",
    "            print(\"samples after\", samples, samples.shape)\n",
    "            sample_grid = make_grid(samples[:,None, :,:,0].detach().cpu(), nrow=int(np.sqrt(config.sampler.n_samples)))\n",
    "\n",
    "            # plot \n",
    "            plt.figure(figsize=(6,6))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(sample_grid.permute(1, 2, 0).cpu())\n",
    "            #plt.show()\n",
    "            # save\n",
    "            saving_plot_path = os.path.join(config.saving.sample_plot_path, f\"samples_epoch_{state['n_iter']}.png\")\n",
    "            plt.savefig(saving_plot_path)\n",
    "            plt.close()\n",
    "            \n",
    "            state['model'].train()\n",
    "\n",
    "        if (state['n_iter'] + 1) % config.saving.checkpoint_freq == 0 or state['n_iter']== config.training.n_iter - 1:\n",
    "            bookkeeping.save_state(state, config.saving.save_location)\n",
    "\n",
    "        if config.training.n_iter == state['n_iter'] - 1:\n",
    "            exit_flag = True\n",
    "            break\n",
    "\n",
    "        state['n_iter'] += 1\n",
    "\n",
    "    print(\"Average Loss after 1 epoch:\", avg_loss / num_items)\n",
    "    if exit_flag:\n",
    "        break\n",
    "                \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
