{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNow, when we minimize LCT, we are sampling (x, x ̃) from the forward process and then maximizing the assigned model probability for \\n\\nthe pairing in the reverse direction, just as in LDT. The slight extra complexity comes from the fact we areconsidering the case \\n\\nwhen xk = xk+1 and the case when xk ̸= xk+1 separately. When xk = xk+1, this corresponds to the first term in LCT which we can see \\n\\nis minimizing the reverse rate out of x which is exactly maximizing the model probability for no transition to occur. When xk ̸= xk+1, \\n\\nthis corresponds to the second term in LCT, which is maximizing the reverse rate from x ̃ to x which in turn maximizes the model probability \\n\\nfor the x ̃ to x transition to occur.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from lib.models.models import UniformRate\n",
    "import ml_collections\n",
    "from config.config_hollow import get_config\n",
    "from lib.utils import utils\n",
    "import torch.nn.functional as F\n",
    "from lib.networks.networks_paul import UNet\n",
    "from lib.networks.hollow import BidirectionalTransformer\n",
    "\n",
    "\"\"\"\n",
    "Now, when we minimize LCT, we are sampling (x, x ̃) from the forward process and then maximizing the assigned model probability for \n",
    "\n",
    "the pairing in the reverse direction, just as in LDT. The slight extra complexity comes from the fact we areconsidering the case \n",
    "\n",
    "when xk = xk+1 and the case when xk ̸= xk+1 separately. When xk = xk+1, this corresponds to the first term in LCT which we can see \n",
    "\n",
    "is minimizing the reverse rate out of x which is exactly maximizing the model probability for no transition to occur. When xk ̸= xk+1, \n",
    "\n",
    "this corresponds to the second term in LCT, which is maximizing the reverse rate from x ̃ to x which in turn maximizes the model probability \n",
    "\n",
    "for the x ̃ to x transition to occur.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters:  4793856\n",
      "1\n",
      "x_embed torch.Size([32, 1024, 512])\n",
      "inputs SA torch.Size([32, 1024, 512])\n",
      "inputs SA torch.Size([32, 1024, 512])\n",
      "l2r_embed torch.Size([32, 1024, 512])\n",
      "r2l_embed torch.Size([32, 1024, 512])\n",
      "torch.Size([32, 1024, 256])\n"
     ]
    }
   ],
   "source": [
    "config =  get_config()\n",
    "bdt = BidirectionalTransformer(config)\n",
    "print(\"number of parameters: \", sum([p.numel() for p in bdt.parameters()]))\n",
    "B = config.data.batch_size\n",
    "D = config.concat_dim\n",
    "S = config.data.S\n",
    "xt= torch.randint(low=0, high=S, size=(B, D), dtype=torch.int)\n",
    "t = 1 * torch.ones((B, ))\n",
    "print(len(t.shape))\n",
    "x_pred = bdt(xt, t)\n",
    "print(x_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2346, -0.0444, -0.4332,  ...,  0.2859, -0.1501, -0.1339],\n",
      "         [-0.0759,  0.1235, -0.3455,  ..., -0.1851,  0.0319, -0.4554],\n",
      "         [-0.2440,  0.0198,  0.0173,  ..., -0.3450, -0.0305,  0.2140],\n",
      "         ...,\n",
      "         [-0.2690, -0.1685, -0.1649,  ...,  0.1022,  0.1818, -0.1784],\n",
      "         [-0.2845, -0.0939, -0.0050,  ..., -0.0414,  0.0242,  0.2500],\n",
      "         [-0.3164,  0.0866, -0.2345,  ..., -0.2994, -0.1716, -0.2480]],\n",
      "\n",
      "        [[-0.2079, -0.0999, -0.1922,  ..., -0.0418,  0.0293,  0.1094],\n",
      "         [-0.4204,  0.1160, -0.2170,  ...,  0.1167,  0.0362, -0.2727],\n",
      "         [-0.3989, -0.1368, -0.1061,  ..., -0.1048,  0.2412,  0.0325],\n",
      "         ...,\n",
      "         [-0.0888, -0.2017, -0.3097,  ..., -0.2005,  0.1180, -0.1309],\n",
      "         [-0.4322,  0.0519, -0.1681,  ..., -0.0252,  0.0458, -0.2747],\n",
      "         [-0.4762,  0.0979, -0.1339,  ...,  0.0347, -0.0223, -0.6121]],\n",
      "\n",
      "        [[-0.5053, -0.0304, -0.3120,  ...,  0.0210,  0.3148, -0.1173],\n",
      "         [-0.4255,  0.0665, -0.2934,  ...,  0.0141, -0.0159, -0.0033],\n",
      "         [-0.1748,  0.2237,  0.0930,  ...,  0.1124,  0.2815,  0.1516],\n",
      "         ...,\n",
      "         [-0.1611,  0.1314, -0.5144,  ...,  0.0103, -0.2661, -0.0141],\n",
      "         [-0.4055,  0.0358, -0.1320,  ...,  0.2320, -0.2055, -0.0581],\n",
      "         [-0.5238,  0.0162, -0.4075,  ...,  0.1131, -0.0455, -0.1909]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0334,  0.0456, -0.4219,  ...,  0.1819,  0.3849, -0.2793],\n",
      "         [-0.0817,  0.3055, -0.3115,  ...,  0.4148,  0.1800, -0.0374],\n",
      "         [-0.0009, -0.0079,  0.0463,  ...,  0.1227, -0.3155,  0.0491],\n",
      "         ...,\n",
      "         [-0.1327,  0.0336, -0.1940,  ...,  0.2113,  0.1821,  0.3302],\n",
      "         [-0.1930,  0.2168, -0.0270,  ..., -0.3099,  0.3019, -0.2570],\n",
      "         [-0.2627,  0.1837, -0.0083,  ..., -0.1961, -0.1013, -0.2359]],\n",
      "\n",
      "        [[-0.1879, -0.0460, -0.1770,  ...,  0.0334,  0.1440,  0.2557],\n",
      "         [-0.1310,  0.0091, -0.4177,  ...,  0.0835,  0.1114,  0.0143],\n",
      "         [-0.3890, -0.0494, -0.1808,  ...,  0.0787,  0.1457,  0.4048],\n",
      "         ...,\n",
      "         [-0.3150, -0.0078,  0.1347,  ...,  0.1567,  0.1341, -0.0392],\n",
      "         [-0.4360,  0.0182, -0.3439,  ..., -0.0863,  0.4790, -0.1168],\n",
      "         [-0.4430,  0.1096, -0.0650,  ..., -0.2176,  0.0052, -0.4407]],\n",
      "\n",
      "        [[-0.3152, -0.1180, -0.4006,  ..., -0.0095,  0.4236, -0.1529],\n",
      "         [-0.3187,  0.3344, -0.1780,  ...,  0.3864,  0.1306, -0.2565],\n",
      "         [ 0.1645,  0.0756, -0.3069,  ..., -0.0096, -0.0477, -0.2108],\n",
      "         ...,\n",
      "         [-0.3458, -0.0191, -0.2350,  ..., -0.0136, -0.0930, -0.4706],\n",
      "         [-0.0841, -0.0796, -0.1117,  ...,  0.0543, -0.1197,  0.1789],\n",
      "         [-0.3173,  0.1268, -0.1667,  ..., -0.2539, -0.1014, -0.3271]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "S=256\n",
    "rate_const = 1\n",
    "\n",
    "\n",
    "cfg.data.S = S\n",
    "device = 'cpu'\n",
    "cfg.model.rate_const = rate_const\n",
    "\n",
    "S = 256\n",
    "B = 64\n",
    "D = 1024\n",
    "model = UniformRate(cfg, 'cpu')\n",
    "unet = UNet(\n",
    "                in_channel=1,\n",
    "                out_channel=1,\n",
    "                channel=32,\n",
    "                channel_multiplier=cfg.model.ch_mult,\n",
    "                n_res_blocks=cfg.model.num_res_blocks,\n",
    "                attn_resolutions=[16],\n",
    "                num_heads=1,\n",
    "                dropout=cfg.model.dropout,\n",
    "                model_output = 'logits',  # 'logits' or 'logistic_pars'\n",
    "                num_classes=S,\n",
    "                x_min_max=(0, 255),\n",
    "                img_size=32,\n",
    "        )\n",
    "t = 1\n",
    "xt= torch.randint(low=0, high=S, size=(B, D), dtype=torch.int)\n",
    "B, D = xt.shape\n",
    "C, H, W = (1, 32, 32)\n",
    "S = 256\n",
    "x = xt.view(B, C, H, W)\n",
    "x_pred = unet(x, t * torch.ones((B,), device=device))\n",
    "x_pred = x_pred.view(B, D, S)\n",
    "print(\"PRED\",x_pred, x_pred.shape)\n",
    "log_p0t = F.log_softmax(x_pred, dim=2)\n",
    "print(log_p0t, log_p0t.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def transformer_timestep_embedding(timesteps, embedding_dim, max_positions=10000):\n",
    "    assert embedding_dim % 2 == 0\n",
    "    assert len(timesteps.shape) == 1\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(max_positions) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = timesteps[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "x = torch.randint(0, S, (B, D))  # Zufällige Integer zwischen 0 und vocab_size - 1\n",
    "\n",
    "# Embedding-Layer\n",
    "embedding = nn.Embedding(S, 10)\n",
    "x = embedding(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, S, (B, D))  # Zufällige Integer zwischen 0 und vocab_size - 1\n",
    "\n",
    "# Embedding-Layer\n",
    "embedding = nn.Embedding(S, 10)\n",
    "x = embedding(x)\n",
    "print(x.shape)\n",
    "\n",
    "t =1 * torch.ones((B,))\n",
    "temb = transformer_timestep_embedding(t, 10, max_positions=10000)\n",
    "\n",
    "assert x.ndim == 3 and temb.ndim == 2 # B, D, E and B, E\n",
    "print(temb.shape)\n",
    "temb = temb.unsqueeze(1)\n",
    "print(temb.shape)\n",
    "conditioner = temb\n",
    "\n",
    "#conditioner = torch.cat([conditioner, temb], dim=1) # B, 2D, E\n",
    "print(conditioner.shape)\n",
    "cond_dim = conditioner.size(1) # 2D\n",
    "print(\"cond_dim\", cond_dim)\n",
    "concat_dim = x.size(1) + cond_dim - 1 # 3D -1\n",
    "print(\"conc dim\", concat_dim)\n",
    "pos_idx = torch.arange(concat_dim, dtype=torch.int32).unsqueeze(0)\n",
    "print(\"pos\", pos_idx.shape)\n",
    "print(\"x[:, :-1]\", x[:, :-1].shape)\n",
    "x = torch.cat([conditioner, x[:, :-1]], dim=1)\n",
    "print(\"conditioner\", conditioner.shape)\n",
    "print(\"x\", x.shape)\n",
    "mask = pos_idx.unsqueeze(-1) <= pos_idx.unsqueeze(-2)\n",
    "mask = mask.unsqueeze(-3) \n",
    "print(\"mask\", mask, mask.shape)\n",
    "mask = mask[:, :, -cond_dim:, -cond_dim:] = 1.0\n",
    "print(\"mask\", mask, mask.shape)\n",
    "#attn_mask = torch.triu(torch.ones((concat_dim, concat_dim)), diagonal=1).bool()\n",
    "attn_mask = torch.tril(torch.ones(concat_dim, concat_dim), diagonal=1).bool()\n",
    "print(\"mask\", mask, mask.shape)\n",
    "print(mask.squeeze(0).squeeze(0) == ~attn_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = torch.triu(torch.ones((5, 5)), diagonal=1).bool()\n",
    "print(attn_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = torch.tril(torch.ones(5, 5), diagonal=-1).bool()\n",
    "print(attn_mask)\n",
    "#print(attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt_test = model.transition\n",
    "print(qt_test.shape)\n",
    "qt_test = utils.expand_dims(qt_test, axis=list(range(1, xt.dim() - 1)))\n",
    "print(qt_test.shape)\n",
    "torch.where(qt_test <= 1e-35, -1e9, torch.log(qt_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.01\n",
    "start_opt = time.time()\n",
    "t_eps = t - h #tau\n",
    "q_teps_0 = model.transition(t_eps * torch.ones((B,), device=device)) # (N, S, S)\n",
    "print(q_teps_0)\n",
    "q_teps_0 = utils.expand_dims(q_teps_0, axis=list(range(1, xt.ndim)))\n",
    "\n",
    "\n",
    "q_t_teps = model.transit_between(t_eps * torch.ones((B,), device=device), t * torch.ones((B,), device=device))  # (N, S, S\n",
    "print(q_t_teps)\n",
    "q_t_teps = q_t_teps.permute(0, 2, 1)\n",
    "b = utils.expand_dims(torch.arange(xt.shape[0]), axis=list(range(1, xt.ndim)))\n",
    "q_t_teps = q_t_teps[b, xt.long()].unsqueeze(-2)\n",
    "print(\"q_teps_0\", q_teps_0, q_teps_0.shape)\n",
    "print(\"q_t_teps\", q_t_teps, q_t_teps.shape)\n",
    "start_opt = time.time()\n",
    "qt0 = q_teps_0 * q_t_teps \n",
    "print(\"qt0\", qt0, qt0.shape)\n",
    "\n",
    "end_opt = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((qt0 >= 0).all())\n",
    "print(torch.log(qt0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.where(q_teps_0 <= 0.0, -1e9, torch.log(q_teps_0))\n",
    "b = torch.where(q_t_teps <= 0.0, -1e9, torch.log(q_t_teps))\n",
    "c = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = utils.expand_dims(torch.arange(xt.shape[0]), axis=list(range(1, xt.ndim)))\n",
    "q_t_teps = q_t_teps[b, xt.long()].unsqueeze(-2)\n",
    "qt0 = q_teps_0 * q_t_teps # 30-60sekunden\n",
    "\n",
    "log_qt0 = torch.where(qt0 <= 0.0, -1e9, torch.log(qt0)) # 7min\n",
    "start_opt = time.time()\n",
    "log_p0t = log_p0t.unsqueeze(-1)\n",
    "log_prob = torch.logsumexp(log_p0t + log_qt0, dim=-2)\n",
    "end_opt = time.time()\n",
    "print(end_opt - start_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_qt0, log_qt0.shape)\n",
    "print(c, c.shape)\n",
    "print(c == log_qt0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_p0t = log_p0t.unsqueeze(-1)\n",
    "log_prob = torch.logsumexp(log_p0t + log_qt0, dim=-2)\n",
    "# axis kein parameter? fehler hier\n",
    "end_opt = time.time()\n",
    "print(\"sampling operations time\", end_opt - start_opt)\n",
    "q_teps_0 = model.transition(t_eps * torch.ones((B,), device=device)) # (N, S, S)\n",
    "print(q_teps_0, q_teps_0.shape)\n",
    "q_teps_0 = utils.expand_dims(q_teps_0, axis=list(range(1, xt.ndim)))\n",
    "print(q_teps_0, q_teps_0.shape)\n",
    "q_t_teps = model.transit_between(t_eps * torch.ones((B,), device=device), t * torch.ones((B,), device=device))  # (N, S, S\n",
    "print(q_t_teps, q_t_teps.shape)\n",
    "q_t_teps = q_t_teps.permute(0, 2, 1)\n",
    "print(q_t_teps, q_t_teps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = utils.expand_dims(torch.arange(xt.shape[0]), axis=list(range(1, xt.ndim)))\n",
    "print(b, b.shape)\n",
    "q_t_teps = q_t_teps[b, xt].unsqueeze(-2)\n",
    "print(q_t_teps, q_t_teps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Transition matrix q_t|0: x0 -> xt ---------------------\n",
    "cfg = get_config()\n",
    "cfg.data.S = S\n",
    "cfg.model.rate_const = rate_const\n",
    "uni = UniformRate(cfg, 'cpu')\n",
    "ts = torch.rand((B,))\n",
    "qt0 = uni.transition(ts)\n",
    "x0= torch.randint(low=0, high=S, size=(B, D), dtype=torch.int)\n",
    "#print(x0)\n",
    "#print(qt0, qt0.shape)\n",
    "qt0_rows_reg = qt0[\n",
    "    torch.arange(B, device=device).repeat_interleave(\n",
    "        D\n",
    "    ),  # repeats every element 0 to B-1 D-times\n",
    "    x0.flatten().long(),  # minibatch.flatten() => (B, D) => (B*D) (1D-Tensor)\n",
    "    :,\n",
    "]\n",
    "print(qt0_rows_reg, qt0_rows_reg.shape)\n",
    "b = utils.expand_dims(torch.arange(B), (tuple(range(1, x0.dim()))))\n",
    "qt0_rows_reg2 = qt0[b, x0] #.view(-1, S)\n",
    "\n",
    "logits = torch.where(qt0 <= 0.0, -1e9, torch.log(qt0_rows_reg2))\n",
    "\n",
    "\n",
    "x_t_cat = torch.distributions.categorical.Categorical(qt0_rows_reg)\n",
    "x_t = x_t_cat.sample().view(B, D)\n",
    "print(x_t, x_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------- Transition rate: x_t -> x_tilde ------------------\n",
    "rate = uni.rate(ts)\n",
    "#print(rate, rate.shape) # B, S, S\n",
    "rate_vals_square = rate[\n",
    "        torch.arange(B, device=device).repeat_interleave(D), x_t.long().flatten(), :\n",
    "    ]\n",
    "#print(rate_vals_square, rate_vals_square.shape)\n",
    "rate_vals_square[\n",
    "        torch.arange(B * D, device=device), x_t.long().flatten()\n",
    "    ] = 0.0 \n",
    "print(rate_vals_square, rate_vals_square.shape)\n",
    "\n",
    "rate_vals_square = rate_vals_square.view(B, D, S)\n",
    "print(rate_vals_square, rate_vals_square.shape)\n",
    "\n",
    "rate_vals_square_dimsum = torch.sum(rate_vals_square, dim=2).view(B, D)\n",
    "print(rate_vals_square_dimsum, rate_vals_square_dimsum.shape)\n",
    "\n",
    "square_dimcat = torch.distributions.categorical.Categorical(rate_vals_square_dimsum)\n",
    "\n",
    "square_dims = square_dimcat.sample() # sampled where transition takes places in every row of B\n",
    "print(\"Where transition\", square_dims, square_dims.shape)\n",
    "\n",
    "rate_new_val_probs = rate_vals_square[\n",
    "    torch.arange(B, device=device), square_dims, :\n",
    "]  # (B, S)\n",
    "print(rate_new_val_probs, rate_new_val_probs.shape)\n",
    "\n",
    "square_newvalcat = torch.distributions.categorical.Categorical(\n",
    "    rate_new_val_probs\n",
    ")\n",
    "\n",
    "# Shape: (B,) mit Werten im Bereich [0, S)\n",
    "square_newval_samples = (\n",
    "    square_newvalcat.sample()\n",
    ")\n",
    "print(\"Transition value\", square_newval_samples, square_newval_samples.shape)\n",
    "\n",
    "x_tilde = x_t.clone()\n",
    "        # noisy image \n",
    "x_tilde[torch.arange(B, device=device), square_dims] = square_newval_samples\n",
    "print(x_t)\n",
    "print(x_tilde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------ELBO-------------------\n",
    "mask_reg = torch.ones((B, D, S), device=device)\n",
    "\n",
    "mask_reg[\n",
    "    torch.arange(B, device=device).repeat_interleave(D),\n",
    "    torch.arange(D, device=device).repeat(B),\n",
    "    x_tilde.long().flatten(),\n",
    "] = 0.0\n",
    "print(x_tilde)\n",
    "print(mask_reg, mask_reg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt0_numer_reg = qt0.view(B, S, S)\n",
    "print(qt0_numer_reg , qt0_numer_reg.shape)\n",
    "# q_{t|0} (x|x_0)\n",
    "qt0_denom_reg = (\n",
    "    qt0[\n",
    "        torch.arange(B, device=device).repeat_interleave(D),\n",
    "        :,\n",
    "        x_tilde.long().flatten(),\n",
    "    ].view(B, D, S)\n",
    "    + 1e-6\n",
    ")\n",
    "#print(qt0_denom_reg, qt0_denom_reg.shape)\n",
    "\n",
    "#print(rate, rate.shape)\n",
    "rate_vals_reg = rate[\n",
    "    torch.arange(B, device=device).repeat_interleave(D),\n",
    "    :,\n",
    "    x_tilde.long().flatten(),\n",
    "].view(B, D, S)\n",
    "print(rate_vals_reg, rate_vals_reg.shape)\n",
    "print((mask_reg * rate_vals_reg))\n",
    "reg_tmp = (mask_reg * rate_vals_reg) @ qt0_numer_reg.transpose(1, 2)\n",
    "print(reg_tmp, reg_tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_const = 1\n",
    "S = 3\n",
    "B = 2\n",
    "D = 4\n",
    "cfg = get_config()\n",
    "cfg.data.S = S\n",
    "cfg.model.rate_const = rate_const\n",
    "uni = UniformRate(cfg, 'cpu')\n",
    "ts = torch.rand((B,))\n",
    "xt= torch.randint(low=0, high=S, size=(B, D), dtype=torch.int)\n",
    "print(xt)\n",
    "\n",
    "qt0 = uni.transition(ts)\n",
    "\n",
    "qt0_y2x = qt0.permute(0, 2, 1)\n",
    "print(qt0, qt0.shape)\n",
    "print(qt0_y2x, qt0_y2x.shape)\n",
    "print(qt0 == qt0_y2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = utils.expand_dims(\n",
    "    torch.arange(xt.shape[0]), tuple(range(1, xt.dim()))\n",
    ")\n",
    "print(b, b.shape)\n",
    "qt0_y2x = qt0_y2x[b, xt]\n",
    "print(qt0_y2x, qt0_y2x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = qt0_y2x\n",
    "log_p0t = F.log_softmax(logits, dim=-1)\n",
    "print(log_p0t, log_p0t.shape)\n",
    "log_qt0 = torch.where(qt0 <= 1e-35, -1e9, torch.log(qt0))\n",
    "print(log_qt0, log_qt0.shape)\n",
    "log_qt0 = utils.expand_dims(log_qt0, axis=list(range(1, xt.dim())))\n",
    "print(log_qt0, log_qt0.shape)\n",
    "log_p0t = log_p0t.unsqueeze(-1)\n",
    "print(log_p0t, log_p0t.shape)\n",
    "log_prob = torch.logsumexp(log_p0t + log_qt0, dim=-2)\n",
    "print(log_prob, log_prob.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_onehot = F.one_hot(xt.long(), S)\n",
    "qt0 = uni.transition(ts)\n",
    "p0t = F.softmax(logits, dim=-1)\n",
    "print(p0t, p0t.shape)\n",
    "qt0 = utils.expand_dims(qt0, axis=list(range(1, xt.dim() - 1)))\n",
    "print(qt0, qt0.shape)\n",
    "prob_all = p0t @ qt0\n",
    "print(prob_all.shape)\n",
    "log_prob = torch.log(prob_all + 1e-35)\n",
    "print(log_prob, log_prob.shape)\n",
    "log_xt = torch.sum(log_prob * xt_onehot, axis=-1)\n",
    "print(log_xt, log_xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt0 = uni.transition(ts)\n",
    "t_eps = ts - 0.1\n",
    "q_t_teps = uni.transit_between(t_eps * torch.ones((B,), device=device), ts * torch.ones((B,), device=device))\n",
    "print(q_t_teps, q_t_teps.shape, qt0.shape)\n",
    "b = utils.expand_dims(torch.arange(B), (tuple(range(1, x0.dim()))))\n",
    "qt0_rows_reg2 = qt0[b, x0]\n",
    "print(qt0_rows_reg2, qt0_rows_reg2.shape)\n",
    "logits = torch.where(qt0_rows_reg2  <= 0.0, -1e9, torch.log(qt0_rows_reg2))\n",
    "print(logits, logits.shape)\n",
    "\n",
    "x_t_cat = torch.distributions.categorical.Categorical(logits).sample()\n",
    "print(x_t_cat,x_t_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_xt = xt #B, D\n",
    "ll_all =  logits# B, D, S\n",
    "loss = -(\n",
    "    (S - 1) * ll_xt\n",
    "    + torch.sum(utils.log1mexp(ll_all), dim=-1)\n",
    "    - utils.log1mexp(ll_xt)\n",
    ")\n",
    "print(loss, loss.shape)\n",
    "weight = torch.ones((B,), dtype=torch.float32)\n",
    "weight = utils.expand_dims(weight, axis=list(range(1, loss.dim())))\n",
    "print(weight, weight.shape)\n",
    "loss = loss * weight\n",
    "print(loss, loss.shape)\n",
    "loss = torch.sum(loss) / xt.shape[0]\n",
    "print(loss, loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_xt = xt #B, D\n",
    "ll_all =  logits\n",
    "xt_onehot = F.one_hot(xt.long(), num_classes=S)\n",
    "b = utils.expand_dims(torch.arange(xt.shape[0]), tuple(range(1, xt.dim())))\n",
    "print(b, b.shape)\n",
    "qt0_x2y = uni.transition(ts)\n",
    "print(qt0_x2y, qt0_x2y.shape)\n",
    "qt0_y2x = qt0_x2y.permute(0, 2, 1)\n",
    "print(qt0_x2y, qt0_x2y.shape)\n",
    "qt0_y2x = qt0_y2x[b, xt]\n",
    "print(qt0_x2y, qt0_x2y.shape)\n",
    "ll_xt = ll_xt.unsqueeze(-1)\n",
    "print(\"ll\", ll_xt, ll_xt.shape)\n",
    "backwd = torch.exp(ll_all - ll_xt) * qt0_y2x\n",
    "print(backwd , backwd.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_term = torch.sum(backwd * (1 - xt_onehot), dim=-1)\n",
    "print(first_term , first_term.shape)\n",
    "qt0_x2y = qt0_x2y[b, xt]\n",
    "print(qt0_x2y, qt0_x2y.shape)\n",
    "fwd = (ll_xt - ll_all) * qt0_x2y\n",
    "print(fwd, fwd.shape)\n",
    "second_term = torch.sum(fwd * (1 - xt_onehot), dim=-1)\n",
    "print(second_term, second_term.shape)\n",
    "loss = first_term - second_term\n",
    "print(loss, loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.ones((B,), dtype=torch.float32)\n",
    "weight = utils.expand_dims(weight, axis=list(range(1, loss.dim())))\n",
    "print(weight, weight.shape)\n",
    "loss = loss * weight\n",
    "print(loss, loss.shape)\n",
    "loss = torch.sum(loss) / xt.shape[0]\n",
    "print(loss, loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.concatenate((np.linspace(1.0, 1e-3, 1000), np.array([0])))\n",
    "#save_ts = ts[np.linspace(0, len(ts)-2, num_intermediates, dtype=int)]\n",
    "\n",
    "for idx, t in (enumerate(ts[0:-1])):\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
