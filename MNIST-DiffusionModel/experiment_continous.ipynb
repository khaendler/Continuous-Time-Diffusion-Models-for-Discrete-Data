{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import Unet\n",
    "from diffusion_model import DiffusionModel, DiffusionModelExtended, DiffusionModelTest\n",
    "#from diffusion_model_improved import TargetDiffusion, LearnedVarDiffusion\n",
    "import torch\n",
    "from trainer import Trainer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.out_dim 1\n",
      "dims [32, 32, 64, 64]\n",
      "final 32 1 Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(np.arange(0, 10))\n",
    "\n",
    "# you can always set num_classes \n",
    "unet_model = Unet(dim=32, channels=1, dim_mults=(1, 2, 2), resnet_block_groups=2, num_classes=num_classes, use_sinposemb=False) \n",
    "#unet_model = Unet(dim=32, channels=1, dim_mults=(1, 2, 4), resnet_block_groups=8, num_classes=num_classes, use_sinposemb=False, use_learned_var=True)# 8 blocks\n",
    "# for \n",
    "#unet_model = Unet(dim=32, channels=1, dim_mults=(1, 2, 2), resnet_block_groups=2, num_classes=num_classes, use_sinposemb=False, self_condition=True) \n",
    "#optimizer = torch.optim.Adam(unet_model.parameters(), lr=2e-5)\n",
    "optimizer = torch.optim.Adam(unet_model.parameters(), lr=1e-4)\n",
    "\n",
    "#diffusion_model = DiffusionModelExtended(model=unet_model, image_size=32, in_channels=1, timesteps=1000, beta_schedule='linear', loss_weighing=True, min_snr_gamma=5, offset_noise_strength=0.0)\n",
    "#diffusion_model = DiffusionModel(model=unet_model, image_size=32, in_channels=1, timesteps=1000, beta_schedule='linear')\n",
    "diffusion_model = DiffusionModelTest(model=unet_model, image_size=32, in_channels=1, timesteps=1, beta_schedule='linear', use_cfg_me=True)\n",
    "#diffusion_model = TargetDiffusion(model=unet_model, image_size=32, in_channels=1, timesteps=10, beta_schedule='linear', objective='pred_noise', min_snr_loss_weight=True)\n",
    "#diffusion_model = LearnedVarDiffusion(model=unet_model, image_size=32, in_channels=1, timesteps=10, beta_schedule='linear', objective='pred_noise', min_snr_loss_weight=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:   0%|          | 2/938 [00:30<3:54:02, 15.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.0307296514511108\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:   0%|          | 2/938 [00:29<3:48:02, 14.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.0486416816711426\n",
      "We sample the following classes: \n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Time Step:: 1it [00:01,  1.21s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DiffusionModelTest.sample() got an unexpected keyword argument 'ema_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(diffusion_model\u001b[39m=\u001b[39mdiffusion_model, optimizer\u001b[39m=\u001b[39moptimizer, use_ema\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, use_guided_diff\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, nb_epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain_loop()\n\u001b[1;32m      3\u001b[0m \u001b[39m# model saved with this:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# unet_model = Unet(dim=128, channels=1, dim_mults=(1, 2, 4), resnet_block_groups=8, num_classes=num_classes, use_sinposemb=False) \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# diffusion_model = DiffusionModel(model=unet_model, image_size=32, in_channels=1, timesteps=1000, beta_schedule='linear')\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# trainer.load(\"checkpoints/saved_epoch_1_model.pt\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# trainer.sampling(epoch=1, n_samples=16)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mtrainer.load(\"checkpoints/epoch_0_model.pt\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mtrainer.sampling(epoch=1)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint(\"trainer.diffusion_model.timesteps\", trainer.diffusion_model.timesteps)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/PythonRepositories/Master-Thesis/MNIST-DiffusionModel/trainer.py:161\u001b[0m, in \u001b[0;36mTrainer.train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39m# save generated images\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m (epoch_plus1 \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_epoch \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    159\u001b[0m     epoch_plus1 \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_epochs\n\u001b[1;32m    160\u001b[0m ):\n\u001b[0;32m--> 161\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msampling(\n\u001b[1;32m    162\u001b[0m         epoch_plus1, cond_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcond_weight, sample_random\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    163\u001b[0m     )\n",
      "File \u001b[0;32m~/PythonRepositories/Master-Thesis/MNIST-DiffusionModel/trainer.py:212\u001b[0m, in \u001b[0;36mTrainer.sampling\u001b[0;34m(self, epoch, n_samples, cond_weight, sample_random)\u001b[0m\n\u001b[1;32m    208\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiffusion_model\u001b[39m.\u001b[39msample(\n\u001b[1;32m    209\u001b[0m         n_samples\u001b[39m=\u001b[39mn_samples, classes\u001b[39m=\u001b[39mclasses, cond_weight\u001b[39m=\u001b[39mcond_weight\n\u001b[1;32m    210\u001b[0m     )\n\u001b[1;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ema:\n\u001b[0;32m--> 212\u001b[0m         samples_ema \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdiffusion_model\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m    213\u001b[0m             n_samples\u001b[39m=\u001b[39;49mn_samples,\n\u001b[1;32m    214\u001b[0m             ema_model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mema_model,\n\u001b[1;32m    215\u001b[0m             classes\u001b[39m=\u001b[39;49mclasses,\n\u001b[1;32m    216\u001b[0m             cond_weight\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    217\u001b[0m         )\n\u001b[1;32m    219\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiffusion_model\u001b[39m.\u001b[39msample(\n\u001b[1;32m    221\u001b[0m         n_samples\u001b[39m=\u001b[39mn_samples, classes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cond_weight\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    222\u001b[0m     )\n",
      "File \u001b[0;32m~/PythonRepositories/Master-Thesis/diffvenv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: DiffusionModelTest.sample() got an unexpected keyword argument 'ema_model'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(diffusion_model=diffusion_model, optimizer=optimizer, use_ema=True, use_guided_diff=True, nb_epochs=2, batch_size=64, device='cpu')\n",
    "trainer.train_loop()\n",
    "# model saved with this:\n",
    "# unet_model = Unet(dim=128, channels=1, dim_mults=(1, 2, 4), resnet_block_groups=8, num_classes=num_classes, use_sinposemb=False) \n",
    "# diffusion_model = DiffusionModel(model=unet_model, image_size=32, in_channels=1, timesteps=1000, beta_schedule='linear')\n",
    "# trainer.load(\"checkpoints/saved_epoch_1_model.pt\")\n",
    "# trainer.sampling(epoch=1, n_samples=16)\n",
    "\n",
    "\"\"\"\n",
    "trainer.load(\"checkpoints/epoch_0_model.pt\")\n",
    "trainer.sampling(epoch=1)\n",
    "trainer.nb_epochs = 2\n",
    "trainer.train_loop()\n",
    "trainer.load(\"checkpoints/epoch_1_model.pt\")\n",
    "print(\"trainer.diffusion_model.timesteps\", trainer.diffusion_model.timesteps)\n",
    "\"\"\"\n",
    "# right now: training with dome classifier free guidance implementation => had a mistake in rand_zeros(x)\n",
    "# right now: training extend without offset-noise\n",
    "# right now: training with dome classifier free guidance implementation \n",
    "# ToDo: right now: training extend without offset-noise => noise reingeben anstatt x_0\n",
    "# rn: old cfg method => dna-dif\n",
    "# late nite: dome method mit scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image = torch.randn(10, 1, 32, 32)\n",
    "t = (torch.rand(10) * 10).long()\n",
    "unet = Unet(dim=128, dim_mults=(1, 2, 2), channels=1, resnet_block_groups=8, learned_sinusoidal_dim=18, num_classes=None)\n",
    "num_param = sum([p.numel() for p in unet.parameters()])\n",
    "print(num_param)\n",
    "x_after = unet(image, t)\n",
    "print(x_after.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
