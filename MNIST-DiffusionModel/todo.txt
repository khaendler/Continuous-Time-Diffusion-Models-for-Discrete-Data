time difference in continous
diffusion model für one hot encoding

testen wie gut es ist mit StandardModel => also scaling

=> anderen Model => learned Var, TargetDiffusion x0, v als Backup

=> Implicit Model implementieren?
=> One hot encoding

Prüfen, wieso bit_diffusion schlecht ist

# TO DO:
Verstehen wieso vereinfachung von DDPM Paper nicht genutzt wird, vll dazu  nochmal video

Solving following Questions:

Classifier Free Guidance:
-   Forward: Setting with p_uncond some class labels to 0 => but 0 is class label: print y in trainer next time to see if 0 is a label
    Aber in Classifier Free Guidance steht folgendes:

    We use a single neural network to parameterize both models, where for the unconditional model 
    we can simply input zeros for the class identifier c when predicting the score, i.e. εθ (zλ ) = εθ (zλ , c = 0)

    FRAGEN

    => in Classifier Free Guidance from lucidrains => they do nothin in forward => mabye also
-   Mabye switch to Version from Dome => one change in sample, one in forward()

Self-Conditioning:
in Bit-Diffusion:

Frage1:
noise_level = self.log_snr(times) # => sowas wie beta_t
if torch.rand((1)) < 0.5:
    with torch.no_grad():
        self_cond = self.model(noised_img, noise_level).detach_()

pred = self.model(noised_img, noise_level, self_cond)

=> hier ist self_cond = Noise
=> ich hätte jetzt egal, ob ich noise oder x0 predicte immer x0 mit reingegeben
    => so wird es auch in Lucidrains denoising_diffusion_pytorch.py gemacht: https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py
was ist richtig?

Frage2: Kombination Self-conditioning und Classifier Free Guidance
=> nehme ich dann diese Formel, wo ich die Prediction von uncond, cond kombiniere:  ̃εθ(zλ, c) = (1 + w)εθ(zλ, c) − wεθ(zλ)
    => nehme das eigentlich ja nur beim Sampling
=> oder nehme ich: 
    if random() < p_uncond:
        classes = None
        x_self_cond_noise = self.model(x=x_noisy, time=t, classes=classes)
        x_self_cond_x0 = ..... * x_self_cond_noise + ....

=> Testen, welche Classifier Free Guidance version besser, dann noise, weigh, self-conditioning mit einfacher prediction


